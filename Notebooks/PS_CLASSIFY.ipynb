{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 0: Imports\n",
    "import os, sys\n",
    "import glob\n",
    "import time\n",
    "import yaml\n",
    "import requests\n",
    "import time\n",
    "import zipfile\n",
    "import shutil\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "import backoff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"Scripts\")))\n",
    "\n",
    "from clip_ALPOD_to_SR_extent import clip_vector_with_geometry, extract_geospatial_info_from_xml\n",
    "from mask_clouds_and_classify_ice import calculate_output_rasters\n",
    "from calculate_ice_cover_statistics_per_lake import calculate_lake_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Config for lake classification\n",
    "\n",
    "config = {\n",
    "    #UDM mask bands to remove data beneath\n",
    "    'udm_mask_bands': [3, 4, 5, 6], # 3 - Shadow, 4 - Light Haze, 5 - Heavy Haze (Depracated but kept in case any are UDM 2.0), 6 - Cloud\n",
    "   \n",
    "    #SR image bands to keep in the final TIF files\n",
    "    'sr_keep_bands': [3], #3 - Red\n",
    "\n",
    "    'pixel_reflectance_thresholds': {\n",
    "        'Ice': (950, 3800),  # Could change to 1100-- silty problem persists\n",
    "        'Snow': (3800, float('inf')),\n",
    "        'Water': (float('-inf'), 950)\n",
    "    }\n",
    "}\n",
    "\n",
    "study_sites_to_process = {\n",
    "    'YF': [\n",
    "        r\"E:\\planetscope_lake_ice\\Data\\Input\\YF 50x50 km\\Breakup_2019\", \n",
    "        r\"E:\\planetscope_lake_ice\\Data\\Input\\YF 50x50 km\\Breakup_2020\",\n",
    "        r\"E:\\planetscope_lake_ice\\Data\\Input\\YF 50x50 km\\Breakup_2021\", \n",
    "        r\"E:\\planetscope_lake_ice\\Data\\Input\\YF 50x50 km\\Breakup_2022\",\n",
    "        r\"E:\\planetscope_lake_ice\\Data\\Input\\YF 50x50 km\\Breakup_2023\", \n",
    "        r\"E:\\planetscope_lake_ice\\Data\\Input\\YF 50x50 km\\Breakup_2024\", \n",
    "        r\"E:\\planetscope_lake_ice\\Data\\Input\\YF 50x50 km\\Breakup_2025\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "output_path = r\"E:\\planetscope_lake_ice\\Data\\Output\"\n",
    "\n",
    "alpod_vector_shapefiles = {\n",
    "    'YF': r\"E:\\planetscope_lake_ice\\Data\\Input\\YF 50x50 km\\YF Lakes from ALPOD Shapefile\\YF_50x50km_lakes.shp\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Image Processing Function \n",
    "\n",
    "def process_planetscope_image(sr_image_path, study_site, lake_vector_shapefile, site_output_path):\n",
    "    \"\"\"\n",
    "    For the given PlanetScope SR TIFF:\n",
    "      0) build output folders, and find its accompanying UDM & XML\n",
    "      1) delete everything but sr_keep_band to reduce size of TIFs in deep storage, create cloud mask as uint 8 file, create classified mask as a uint8 file\n",
    "      2) mask red band, threshold/classify ice & snow\n",
    "      3) append lake stats to NetCDF\n",
    "    \"\"\"\n",
    "    \n",
    "    # ────────────────────────────────────────────────────────────────────────────────\n",
    "    # 0: Create output folders // find correlated XML & UDM for this SR image\n",
    "    # ────────────────────────────────────────────────────────────────────────────────\n",
    "    # build image name strings for processing dates\n",
    "    sr_filename = os.path.basename(sr_image_path) # E.g. 20200415_222212_87_1060_ortho_analytic_4b_sr.tif\n",
    "    sr_filename_no_ext = os.path.splitext(sr_filename)[0] # E.g. 20200415_222212_87_1060_ortho_analytic_4b_sr\n",
    "    image_core_name = sr_filename_no_ext.split('_ortho', 1)[0] ## E.g. 20200415_222212_87_1060\n",
    "\n",
    "    # Get the full directory path where the SR image is located\n",
    "    sr_image_directory = os.path.dirname(sr_image_path)\n",
    "    \n",
    "    # identify the current season folder (e.g. \"Breakup_2019\") and build output paths based on that season\n",
    "    input_season_folder = os.path.basename(sr_image_directory) # e.g. \"Breakup_2019 from downloads - read from global config\"\n",
    "    output_rasters_dir = os.path.join(site_output_path, \"Rasters\", input_season_folder) \n",
    "    output_shapefile_dir = os.path.join(site_output_path, \"Shapefiles\", input_season_folder, sr_filename_no_ext)\n",
    "    for d in (output_rasters_dir, output_shapefile_dir): \n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # locate UDM and XML metadata using glob to find matching files in the same directory\n",
    "    udm_pattern = os.path.join(sr_image_directory, f\"{image_core_name}*udm2.tif\")\n",
    "    xml_pattern = os.path.join(sr_image_directory, f\"{image_core_name}*.xml\")\n",
    "    \n",
    "    udm_files = glob.glob(udm_pattern)\n",
    "    xml_files = glob.glob(xml_pattern)\n",
    "    \n",
    "    if not udm_files or not xml_files:\n",
    "        raise FileNotFoundError(f\"\\n######################\\nERROR: UDM or XML files not found for {sr_filename}\\nLooked for UDM: {udm_pattern}\\nLooked for XML: {xml_pattern}\\n######################\\n\")\n",
    "    \n",
    "    udm_path = udm_files[0]  # Take the first match\n",
    "    xml_path = xml_files[0]  # Take the first match\n",
    "\n",
    "    print(f\"All pre-processing complete. Beginning classification for image {sr_filename}.\\n\")\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────────────────\n",
    "    # 1: Clip study site lake mask to the image footprint (sourced from the XML file) so only lakes\n",
    "    #  which fall entirely within this SR image's extent are considered\n",
    "    # ────────────────────────────────────────────────────────────────────────────────\n",
    "    output_vector_path = os.path.join(output_shapefile_dir, f\"{image_core_name}_lakes.shp\") # The lakes which fall within the image's extent\n",
    "\n",
    "    print(\"Clipping geometry...\")\n",
    "    geo_info = extract_geospatial_info_from_xml(xml_path)\n",
    "\n",
    "    clip_vector_with_geometry(\n",
    "        lake_vector_shapefile,\n",
    "        geo_info['geometry'],\n",
    "        output_vector_path\n",
    "    )\n",
    "\n",
    "    print(\"Geometry clipped successfully.\\n\")\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────────────────\n",
    "    # 2: Delete everything but sr_keep_band to reduce size of TIFs in deep storage, create cloud mask as uint 8 file, create classified mask as a uint8 file, all with lzw compression\n",
    "    # ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    output_sr_path = os.path.join(output_rasters_dir, \"Single_Band_Rasters_Uint16\", sr_filename)\n",
    "    output_udm_path = os.path.join(output_rasters_dir, \"Cloud_Masks_Uint8\", f\"{image_core_name}_cloud_mask.tif\")\n",
    "    output_classified_path = os.path.join(output_rasters_dir, \"Ice_Snow_Water_Classified_Masks_Uint8\", f\"{image_core_name}_classified_ice_snow.tif\")\n",
    "    \n",
    "    # Make sure the subdirectories exist\n",
    "    for subdir_path in [os.path.dirname(output_sr_path), os.path.dirname(output_udm_path), os.path.dirname(output_classified_path)]:\n",
    "        os.makedirs(subdir_path, exist_ok=True)\n",
    "    \n",
    "    print(\"Creating cloud mask, clipping rasters...\")\n",
    "    calculate_output_rasters(\n",
    "        sr_image_path,\n",
    "        udm_path,\n",
    "        config['udm_mask_bands'],  # Fixed: was 'mask_bands', should be 'udm_mask_bands'\n",
    "        config['sr_keep_bands'],   # Fixed: was 'keep_bands', should be 'sr_keep_bands'\n",
    "        output_sr_path,\n",
    "        output_udm_path,\n",
    "        output_classified_path\n",
    "    )\n",
    "\n",
    "    print(f\"     Clouds masked successfully.\")\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────────────────\n",
    "    # 3: Calculate statistics for each lake, and then add the statistics to an excel sheet\n",
    "    # ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    print(f\"Calculating lake statistics...\")\n",
    "    calculate_lake_statistics(\n",
    "        output_classified_path,\n",
    "        image_core_name,\n",
    "        site_output_path,\n",
    "        study_site,\n",
    "        lake_vector_shapefile\n",
    "    )\n",
    "\n",
    "    print(f\"Finished processing {image_core_name}\\n# ──────────────────────────────────────────────────────────────────────────────── #\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Loop through all images to clip, clean, and classify lake ice cover\n",
    "\n",
    "for study_site, paths_list in study_sites_to_process.items():\n",
    "    lake_vector_shapefile = alpod_vector_shapefiles[study_site]  # Get corresponding shapefile\n",
    "\n",
    "    for site_folder in paths_list:\n",
    "        pattern = os.path.join(site_folder, \"*.tif\")  # Match all .tif files\n",
    "        for sr_tif_path in glob.glob(pattern):\n",
    "            if 'udm' not in os.path.basename(sr_tif_path).lower():  # Exclude files with 'udm' so only SR goes through\n",
    "                try:\n",
    "                    site_output_path = os.path.join(output_path, study_site)\n",
    "\n",
    "                    # Then pass it to the function\n",
    "                    process_planetscope_image(sr_tif_path, study_site, lake_vector_shapefile, site_output_path)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {sr_tif_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 lake CSVs...\n",
      "Final CSV saved to E:\\planetscope_lake_ice\\Data\\Output\\YF\\lake_breakup_results_YF.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def parse_histogram(histogram_str):\n",
    "    try:\n",
    "        histogram = json.loads(histogram_str)\n",
    "        total_pixels = sum(histogram.values())\n",
    "        invalid_pixels = histogram.get('0', 0) + histogram.get('255', 0)\n",
    "        invalid_percent = (invalid_pixels / total_pixels) * 100 if total_pixels > 0 else 100\n",
    "\n",
    "        ice_pixels = histogram.get('1', 0)\n",
    "        snow_pixels = histogram.get('2', 0)\n",
    "        water_pixels = histogram.get('3', 0)\n",
    "\n",
    "        valid_pixels = ice_pixels + snow_pixels + water_pixels\n",
    "        if valid_pixels == 0:\n",
    "            return 0, 0, 0, invalid_percent, False\n",
    "\n",
    "        return (\n",
    "            (ice_pixels / valid_pixels) * 100,\n",
    "            (snow_pixels / valid_pixels) * 100,\n",
    "            (water_pixels / valid_pixels) * 100,\n",
    "            invalid_percent,\n",
    "            True\n",
    "        )\n",
    "    except (json.JSONDecodeError, ValueError, ZeroDivisionError):\n",
    "        return 0, 0, 0, 100, False\n",
    "\n",
    "def aggregate_daily_observations(df):\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    daily_data = []\n",
    "    for date, group in df.groupby('date'):\n",
    "        max_water_idx = group['water_percent'].idxmax()\n",
    "        best_obs = group.loc[max_water_idx]\n",
    "        datetime_repr = best_obs['datetime'].replace(hour=12, minute=0, second=0, microsecond=0)\n",
    "        daily_data.append({\n",
    "            'datetime': datetime_repr,\n",
    "            'date': date,\n",
    "            'ice_percent': best_obs['ice_percent'],\n",
    "            'snow_percent': best_obs['snow_percent'],\n",
    "            'water_percent': best_obs['water_percent']\n",
    "        })\n",
    "    return pd.DataFrame(daily_data).sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "def classify_pixel_validity(df_daily):\n",
    "    df_daily['pixel_valid'] = True\n",
    "\n",
    "    for i, row in df_daily.iterrows():\n",
    "        if row['ice_percent'] < 10 and row['snow_percent'] < 10 and row['water_percent'] < 10:\n",
    "            df_daily.loc[i, 'pixel_valid'] = False\n",
    "\n",
    "    for i, row in df_daily.iterrows():\n",
    "        if sum([row['ice_percent'] == 100, row['snow_percent'] == 100, row['water_percent'] == 100]) > 1:\n",
    "            df_daily.loc[i, 'pixel_valid'] = False\n",
    "\n",
    "    if len(df_daily) >= 3:\n",
    "        for i in range(1, len(df_daily) - 1):\n",
    "            if not df_daily.loc[df_daily.index[i - 1], 'pixel_valid']:\n",
    "                continue\n",
    "            if not df_daily.loc[df_daily.index[i + 1], 'pixel_valid']:\n",
    "                continue\n",
    "            for class_name in ['ice_percent', 'snow_percent']:\n",
    "                prev_val = df_daily.loc[df_daily.index[i - 1], class_name]\n",
    "                curr_val = df_daily.loc[df_daily.index[i], class_name]\n",
    "                next_val = df_daily.loc[df_daily.index[i + 1], class_name]\n",
    "                if (curr_val - prev_val) > 75 and (curr_val - next_val) > 75:\n",
    "                    if abs(prev_val - next_val) < abs(curr_val - prev_val):\n",
    "                        df_daily.loc[df_daily.index[i], 'pixel_valid'] = False\n",
    "                        break\n",
    "\n",
    "    first_high_water_idx = None\n",
    "    for i, row in df_daily.iterrows():\n",
    "        if row['pixel_valid'] and row['water_percent'] >= 75:\n",
    "            first_high_water_idx = row.name\n",
    "            break\n",
    "\n",
    "    if first_high_water_idx is not None:\n",
    "        start_pos = df_daily.index.get_loc(first_high_water_idx)\n",
    "        for i in range(start_pos + 1, len(df_daily)):\n",
    "            if df_daily.loc[df_daily.index[i], 'water_percent'] < 95:\n",
    "                df_daily.loc[df_daily.index[i], 'pixel_valid'] = False\n",
    "\n",
    "    return df_daily\n",
    "\n",
    "def find_breakup_date(df_sorted):\n",
    "    df_daily = aggregate_daily_observations(df_sorted)\n",
    "    df_classified = classify_pixel_validity(df_daily)\n",
    "    valid_df = df_classified[df_classified['pixel_valid']].reset_index(drop=True)\n",
    "\n",
    "    for i in range(len(valid_df) - 1):\n",
    "        curr = valid_df.iloc[i]\n",
    "        next_ = valid_df.iloc[i + 1]\n",
    "        if curr['water_percent'] <= 75 < next_['water_percent']:\n",
    "            if next_['water_percent'] == curr['water_percent']:\n",
    "                continue\n",
    "            t1 = curr['datetime']\n",
    "            t2 = next_['datetime']\n",
    "            frac = (75 - curr['water_percent']) / (next_['water_percent'] - curr['water_percent'])\n",
    "            breakup_date = t1 + (t2 - t1) * frac\n",
    "            lower_bound_days = round((breakup_date - t1).total_seconds() / 86400)\n",
    "            upper_bound_days = round((t2 - breakup_date).total_seconds() / 86400)\n",
    "            return breakup_date, lower_bound_days, upper_bound_days, \"WATER interpolated @75%\"\n",
    "\n",
    "    return None, None, None, \"NO CROSSING FOUND\"\n",
    "\n",
    "def calculate_breakup_stats_continuous_multiyear(csv_folder_path, output_csv_path, cloud_threshold=33):\n",
    "    csv_files = glob.glob(os.path.join(csv_folder_path, \"*_ice_snow.csv\"))\n",
    "    results = []\n",
    "\n",
    "    print(f\"Found {len(csv_files)} lake CSVs...\")\n",
    "\n",
    "    for file_index, csv_file in enumerate(csv_files):\n",
    "        lake_id = int(os.path.basename(csv_file).split('_')[0])\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            valid_data = []\n",
    "            for _, r in df.iterrows():\n",
    "                ice_pct, snow_pct, water_pct, invalid_pct, is_valid_hist = parse_histogram(r['histogram'])\n",
    "                if is_valid_hist and invalid_pct <= cloud_threshold:\n",
    "                    timestamp = datetime.datetime.fromtimestamp(r['unix_time'])\n",
    "                    valid_data.append({\n",
    "                        'datetime': timestamp,\n",
    "                        'ice_percent': ice_pct,\n",
    "                        'snow_percent': snow_pct,\n",
    "                        'water_percent': water_pct\n",
    "                    })\n",
    "\n",
    "            if not valid_data:\n",
    "                results.append({\n",
    "                    'lake_id': lake_id,\n",
    "                    'year': None,\n",
    "                    'breakup_date': None,\n",
    "                    'error_days_negative': None,\n",
    "                    'error_days_positive': None,\n",
    "                    'total_observations': len(df),\n",
    "                    'valid_observations': 0,\n",
    "                    'cloud_threshold_used': cloud_threshold,\n",
    "                    'detection_method': f'ALL IMAGES > {cloud_threshold}% INVALID'\n",
    "                })\n",
    "                pd.DataFrame(results).to_csv(output_csv_path, index=False)\n",
    "                continue\n",
    "\n",
    "            df_valid = pd.DataFrame(valid_data)\n",
    "            df_valid['year'] = df_valid['datetime'].dt.year\n",
    "            df_valid['month'] = df_valid['datetime'].dt.month\n",
    "\n",
    "            for year, year_group in df_valid.groupby('year'):\n",
    "                # Only use spring months April–June\n",
    "                spring_data = year_group[year_group['month'].between(4, 6)].sort_values('datetime')\n",
    "                if spring_data.empty:\n",
    "                    detection_method = 'NO SPRING DATA'\n",
    "                    results.append({\n",
    "                        'lake_id': lake_id,\n",
    "                        'year': year,\n",
    "                        'breakup_date': None,\n",
    "                        'error_days_negative': None,\n",
    "                        'error_days_positive': None,\n",
    "                        'total_observations': len(year_group),\n",
    "                        'valid_observations': 0,\n",
    "                        'cloud_threshold_used': cloud_threshold,\n",
    "                        'detection_method': detection_method\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                df_daily = aggregate_daily_observations(spring_data)\n",
    "                if df_daily.empty:\n",
    "                    detection_method = 'NO VALID DAILY DATA'\n",
    "                    results.append({\n",
    "                        'lake_id': lake_id,\n",
    "                        'year': year,\n",
    "                        'breakup_date': None,\n",
    "                        'error_days_negative': None,\n",
    "                        'error_days_positive': None,\n",
    "                        'total_observations': len(spring_data),\n",
    "                        'valid_observations': 0,\n",
    "                        'cloud_threshold_used': cloud_threshold,\n",
    "                        'detection_method': detection_method\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                first_row = df_daily.iloc[0]\n",
    "                if not (first_row['ice_percent'] > first_row['water_percent'] or first_row['snow_percent'] > first_row['water_percent']):\n",
    "                    detection_method = 'INVALID TIME SERIES - NO ICE START'\n",
    "                    results.append({\n",
    "                        'lake_id': lake_id,\n",
    "                        'year': year,\n",
    "                        'breakup_date': None,\n",
    "                        'error_days_negative': None,\n",
    "                        'error_days_positive': None,\n",
    "                        'total_observations': len(spring_data),\n",
    "                        'valid_observations': len(df_daily),\n",
    "                        'cloud_threshold_used': cloud_threshold,\n",
    "                        'detection_method': detection_method\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                breakup_date, before_days, after_days, detection_method = find_breakup_date(spring_data)\n",
    "                results.append({\n",
    "                    'lake_id': lake_id,\n",
    "                    'year': year,\n",
    "                    'breakup_date': breakup_date.strftime('%m/%d/%Y %I:%M:%S %p') if breakup_date else None,\n",
    "                    'error_days_negative': before_days,\n",
    "                    'error_days_positive': after_days,\n",
    "                    'total_observations': len(spring_data),\n",
    "                    'valid_observations': len(df_daily),\n",
    "                    'cloud_threshold_used': cloud_threshold,\n",
    "                    'detection_method': detection_method\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {lake_id}: {e}\")\n",
    "\n",
    "        # Incremental write after every lake\n",
    "        pd.DataFrame(results).to_csv(output_csv_path, index=False)\n",
    "\n",
    "        if (file_index + 1) % 50 == 0:\n",
    "            print(f\"Processed {file_index + 1}/{len(csv_files)} lakes\")\n",
    "\n",
    "    print(f\"Final CSV saved to {output_csv_path}\")\n",
    "\n",
    "# Example run\n",
    "for study_site in ['YF']:\n",
    "    csv_folder = os.path.join(output_path, study_site, \"Lake Time Series CSVs\")\n",
    "    output_csv = os.path.join(output_path, study_site, f\"lake_breakup_results_{study_site}.csv\")\n",
    "    calculate_breakup_stats_continuous_multiyear(csv_folder, output_csv, cloud_threshold=33)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
