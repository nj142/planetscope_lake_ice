{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e323c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Planet API Satellite Image Downloader - Threaded Version\n",
    "\n",
    "Enhanced with safe threading for faster processing:\n",
    "- Batch activation of all assets\n",
    "- Activation checking queue with 5-minute intervals\n",
    "- Download queue for activated assets\n",
    "- Thread-safe CSV updates\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import yaml\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import glob\n",
    "import tempfile\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "import json\n",
    "import threading\n",
    "from queue import Queue, Empty\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from filelock import FileLock  # pip install filelock\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# API configuration file\n",
    "PLANET_YAML_PATH = r\"D:\\planetscope_lake_ice\\planet.yaml\"\n",
    "\n",
    "# Load API key\n",
    "with open(PLANET_YAML_PATH, 'r') as f:\n",
    "    PLANET_API_KEY = yaml.safe_load(f)['api_key']\n",
    "\n",
    "BASE_URL = \"https://api.planet.com/data/v1\"\n",
    "\n",
    "# Asset configuration with fallback mapping\n",
    "ASSET_CONFIG = {\n",
    "    'ortho_analytic_4b_sr': {\n",
    "        'fallback': 'ortho_analytic_3b',\n",
    "        'extension': '.tif'\n",
    "    },\n",
    "    'ortho_analytic_4b_xml': {\n",
    "        'fallback': 'ortho_analytic_3b_xml', \n",
    "        'extension': '.xml'\n",
    "    },\n",
    "    'ortho_udm2': {\n",
    "        'fallback': 'ortho_udm2',\n",
    "        'extension': '.tif'\n",
    "    }\n",
    "}\n",
    "\n",
    "ASSET_KEYS = list(ASSET_CONFIG.keys())\n",
    "\n",
    "# Threading configuration\n",
    "MAX_REQUESTS_PER_SECOND = 4\n",
    "REQUEST_INTERVAL = 1.0 / MAX_REQUESTS_PER_SECOND\n",
    "MAX_CONCURRENT_DOWNLOADS = 4  # Number of concurrent download threads\n",
    "MAX_CONCURRENT_ACTIVATIONS = 8  # Number of concurrent activation threads\n",
    "\n",
    "# Timing configuration\n",
    "ACTIVATION_CHECK_INTERVAL = 300  # 5 minutes in seconds\n",
    "MAX_ACTIVATION_WAIT = 18000  # 5 hours in seconds\n",
    "CLOUD_THRESHOLD = 0.0\n",
    "\n",
    "# Base data folder\n",
    "BASE_DATA_FOLDER = r\"E:\\planetscope_lake_ice\\Data\\Input\"\n",
    "\n",
    "# Configuration variables (SET THESE IN JUPYTER CELLS)\n",
    "CURRENT_STUDY_AREA = None\n",
    "CURRENT_AOI = None\n",
    "CURRENT_SEASONS = None\n",
    "CURRENT_YEARS = None\n",
    "CURRENT_FOLDER = None\n",
    "\n",
    "# =============================================================================\n",
    "# DATA STRUCTURES\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class AssetInfo:\n",
    "    \"\"\"Information about an asset to be activated/downloaded\"\"\"\n",
    "    item_id: str\n",
    "    item_type: str\n",
    "    asset_key: str\n",
    "    actual_asset_key: str\n",
    "    df_index: int\n",
    "    row: pd.Series\n",
    "    activation_url: Optional[str] = None\n",
    "    download_url: Optional[str] = None\n",
    "    activated_at: Optional[datetime] = None\n",
    "    activation_requested_at: Optional[datetime] = None\n",
    "\n",
    "@dataclass\n",
    "class ItemDownloadGroup:\n",
    "    \"\"\"Group of assets for a single item that are ready to download\"\"\"\n",
    "    item_id: str\n",
    "    assets: List[AssetInfo]\n",
    "    df_index: int\n",
    "    row: pd.Series\n",
    "\n",
    "# =============================================================================\n",
    "# THREAD-SAFE CSV OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "class ThreadSafeCSVUpdater:\n",
    "    \"\"\"Thread-safe CSV file updater with file locking\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.lock_path = csv_path + '.lock'\n",
    "        self._local_lock = threading.Lock()\n",
    "    \n",
    "    def update_status(self, df_index: int, asset_key: str, new_status: str):\n",
    "        \"\"\"Thread-safe update of asset status in CSV\"\"\"\n",
    "        with self._local_lock:\n",
    "            with FileLock(self.lock_path, timeout=30):\n",
    "                try:\n",
    "                    # Read current CSV\n",
    "                    df = pd.read_csv(self.csv_path)\n",
    "                    \n",
    "                    # Update status\n",
    "                    status_col = f'{asset_key}_status'\n",
    "                    if df_index < len(df) and status_col in df.columns:\n",
    "                        df.loc[df_index, status_col] = new_status\n",
    "                        \n",
    "                        # Write back to CSV\n",
    "                        df.to_csv(self.csv_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "                        logging.info(f\"Updated {asset_key} status to {new_status} for index {df_index}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error updating CSV: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED RATE LIMITED SESSION\n",
    "# =============================================================================\n",
    "\n",
    "class ThreadSafeRateLimitedSession:\n",
    "    \"\"\"Thread-safe rate-limited requests session\"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_second: float = 4):\n",
    "        self.min_interval = 1.0 / requests_per_second\n",
    "        self.last_request_time = 0\n",
    "        self.request_lock = threading.Lock()\n",
    "        self.session = requests.Session()\n",
    "        self.session.auth = (PLANET_API_KEY, '')\n",
    "        \n",
    "    def request(self, method: str, url: str, **kwargs) -> requests.Response:\n",
    "        \"\"\"Make thread-safe rate-limited request\"\"\"\n",
    "        with self.request_lock:\n",
    "            # Wait if needed\n",
    "            now = time.time()\n",
    "            elapsed = now - self.last_request_time\n",
    "            if elapsed < self.min_interval:\n",
    "                time.sleep(self.min_interval - elapsed)\n",
    "            \n",
    "            # Handle timeout parameter - use provided timeout or default\n",
    "            timeout = kwargs.pop('timeout', 60)\n",
    "            \n",
    "            # Make request with retries\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    response = self.session.request(method, url, timeout=timeout, **kwargs)\n",
    "                    self.last_request_time = time.time()\n",
    "                    return response\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    if attempt == 2:  # Last attempt\n",
    "                        raise e\n",
    "                    logging.warning(f\"Request failed (attempt {attempt + 1}/3): {e}\")\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            \n",
    "            raise requests.exceptions.RequestException(\"All retry attempts failed\")\n",
    "\n",
    "# Global thread-safe session\n",
    "session = ThreadSafeRateLimitedSession(MAX_REQUESTS_PER_SECOND)\n",
    "\n",
    "# =============================================================================\n",
    "# ACTIVATION MANAGER\n",
    "# =============================================================================\n",
    "class ActivationManager:\n",
    "    def __init__(self, csv_updater: ThreadSafeCSVUpdater):\n",
    "        self.csv_updater = csv_updater\n",
    "        self.download_queue = Queue()\n",
    "        self.active_assets: Dict[str, AssetInfo] = {}\n",
    "        self.ready_assets: Dict[str, AssetInfo] = {}\n",
    "        self.failed_assets: Set[str] = set()\n",
    "        self.completed_items: Set[str] = set()\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def request_activation(self, asset_info: AssetInfo):\n",
    "        try:\n",
    "            assets_url = f\"{BASE_URL}/item-types/{asset_info.item_type}/items/{asset_info.item_id}/assets/\"\n",
    "            response = session.request('GET', assets_url)\n",
    "            response.raise_for_status()\n",
    "            assets_data = response.json()\n",
    "            if asset_info.actual_asset_key not in assets_data:\n",
    "                self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, \"Does Not Exist\")\n",
    "                return False\n",
    "            asset_data = assets_data[asset_info.actual_asset_key]\n",
    "            status = asset_data.get('status', 'inactive')\n",
    "            # Already active\n",
    "            if status == 'active' and 'location' in asset_data:\n",
    "                asset_info.download_url = asset_data['location']\n",
    "                key = f\"{asset_info.item_id}_{asset_info.asset_key}\"\n",
    "                with self.lock:\n",
    "                    self.ready_assets[key] = asset_info\n",
    "                self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, \"Ready for Download\")\n",
    "                self._check_item_ready_for_download(asset_info.item_id)\n",
    "                return True\n",
    "            # Request activation\n",
    "            if 'activate' in asset_data.get('_links', {}):\n",
    "                activate_url = asset_data['_links']['activate']\n",
    "                session.request('POST', activate_url).raise_for_status()\n",
    "                asset_info.activation_requested_at = datetime.now()\n",
    "                key = f\"{asset_info.item_id}_{asset_info.asset_key}\"\n",
    "                with self.lock:\n",
    "                    self.active_assets[key] = asset_info\n",
    "                self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, \"Activation Requested\")\n",
    "                return True\n",
    "            else:\n",
    "                self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, \"Cannot Activate\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"[Activation Error] {asset_info.item_id} {asset_info.asset_key}: {e}\")\n",
    "            self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, \"Activation Failed\")\n",
    "            return False\n",
    "    \n",
    "    def check_activations(self):\n",
    "        with self.lock:\n",
    "            assets_to_check = list(self.active_assets.values())\n",
    "        print(f\"[Activation Check] Checking {len(assets_to_check)} pending assets...\")\n",
    "        for asset_info in assets_to_check:\n",
    "            try:\n",
    "                if asset_info.activation_requested_at:\n",
    "                    elapsed = (datetime.now() - asset_info.activation_requested_at).total_seconds()\n",
    "                    if elapsed > MAX_ACTIVATION_WAIT:\n",
    "                        self._handle_activation_timeout(asset_info)\n",
    "                        continue\n",
    "                assets_url = f\"{BASE_URL}/item-types/{asset_info.item_type}/items/{asset_info.item_id}/assets/\"\n",
    "                response = session.request('GET', assets_url)\n",
    "                response.raise_for_status()\n",
    "                assets_data = response.json()\n",
    "                if asset_info.actual_asset_key not in assets_data:\n",
    "                    self._handle_activation_failure(asset_info, \"Asset disappeared\")\n",
    "                    continue\n",
    "                asset_data = assets_data[asset_info.actual_asset_key]\n",
    "                status = asset_data.get('status', 'inactive')\n",
    "                if status == 'active' and 'location' in asset_data:\n",
    "                    asset_info.download_url = asset_data['location']\n",
    "                    key = f\"{asset_info.item_id}_{asset_info.asset_key}\"\n",
    "                    with self.lock:\n",
    "                        del self.active_assets[key]\n",
    "                        self.ready_assets[key] = asset_info\n",
    "                    self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, \"Ready for Download\")\n",
    "                    self._check_item_ready_for_download(asset_info.item_id)\n",
    "            except Exception as e:\n",
    "                print(f\"[Activation Check Error] {asset_info.item_id} {asset_info.asset_key}: {e}\")\n",
    "    \n",
    "    def _handle_activation_timeout(self, asset_info: AssetInfo):\n",
    "        key = f\"{asset_info.item_id}_{asset_info.asset_key}\"\n",
    "        with self.lock:\n",
    "            del self.active_assets[key]\n",
    "            self.failed_assets.add(key)\n",
    "        self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, \"Activation Timeout\")\n",
    "        print(f\"[Activation Timeout] {asset_info.item_id} {asset_info.asset_key}\")\n",
    "    \n",
    "    def _handle_activation_failure(self, asset_info: AssetInfo, reason: str):\n",
    "        key = f\"{asset_info.item_id}_{asset_info.asset_key}\"\n",
    "        with self.lock:\n",
    "            del self.active_assets[key]\n",
    "            self.failed_assets.add(key)\n",
    "        self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, f\"Activation Failed: {reason}\")\n",
    "        print(f\"[Activation Failed] {asset_info.item_id} {asset_info.asset_key}: {reason}\")\n",
    "    \n",
    "    def _check_item_ready_for_download(self, item_id: str):\n",
    "        with self.lock:\n",
    "            item_assets = [a for a in self.ready_assets.values() if a.item_id == item_id]\n",
    "            if len(item_assets) == len(ASSET_KEYS):\n",
    "                self.download_queue.put(ItemDownloadGroup(item_id, item_assets, item_assets[0].df_index, item_assets[0].row))\n",
    "                for asset in item_assets:\n",
    "                    del self.ready_assets[f\"{asset.item_id}_{asset.asset_key}\"]\n",
    "                print(f\"[Download Queue] Item {item_id} all assets ready\")\n",
    "    \n",
    "    def get_activation_status(self) -> dict:\n",
    "        with self.lock:\n",
    "            return {\n",
    "                'active_count': len(self.active_assets),\n",
    "                'ready_count': len(self.ready_assets),\n",
    "                'failed_count': len(self.failed_assets),\n",
    "                'completed_count': len(self.completed_items),\n",
    "                'download_queue_size': self.download_queue.qsize()\n",
    "            }\n",
    "\n",
    "# =============================================================================\n",
    "# DOWNLOAD MANAGER\n",
    "# =============================================================================\n",
    "class DownloadManager:\n",
    "    def __init__(self, csv_updater: ThreadSafeCSVUpdater, activation_manager):\n",
    "        self.csv_updater = csv_updater\n",
    "        self.activation_manager = activation_manager\n",
    "        self.completed_downloads = set()\n",
    "        \n",
    "    def get_next_download_group(self) -> Optional[ItemDownloadGroup]:\n",
    "        try:\n",
    "            return self.activation_manager.download_queue.get(timeout=2)\n",
    "        except Empty:\n",
    "            return None\n",
    "    \n",
    "    def download_item_assets(self, item_group: ItemDownloadGroup) -> bool:\n",
    "        item_id = item_group.item_id\n",
    "        row = item_group.row\n",
    "        try:\n",
    "            season, year = row['season'], row['year']\n",
    "            folder_name = f\"{season}_{year}\"\n",
    "            output_folder = os.path.join(CURRENT_FOLDER, folder_name)\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_zip:\n",
    "                temp_zip_path = temp_zip.name\n",
    "            try:\n",
    "                successful_downloads = []\n",
    "                with zipfile.ZipFile(temp_zip_path, 'w') as zf:\n",
    "                    for asset_info in item_group.assets:\n",
    "                        if not asset_info.download_url:\n",
    "                            continue\n",
    "                        try:\n",
    "                            response = session.request('GET', asset_info.download_url, timeout=300)\n",
    "                            response.raise_for_status()\n",
    "                            if not response.content:\n",
    "                                self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, \"Download Failed - Empty\")\n",
    "                                continue\n",
    "                            extension = ASSET_CONFIG[asset_info.asset_key]['extension']\n",
    "                            filename = f\"{item_id}_{asset_info.actual_asset_key}{extension}\"\n",
    "                            zf.writestr(filename, response.content)\n",
    "                            successful_downloads.append(asset_info)\n",
    "                        except Exception as e:\n",
    "                            print(f\"[Download Error] {asset_info.asset_key} {item_id}: {e}\")\n",
    "                            self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, \"Download Failed\")\n",
    "                if successful_downloads:\n",
    "                    with zipfile.ZipFile(temp_zip_path, 'r') as zf:\n",
    "                        zf.extractall(output_folder)\n",
    "                        for asset_info in successful_downloads:\n",
    "                            extension = ASSET_CONFIG[asset_info.asset_key]['extension']\n",
    "                            expected_file = os.path.join(output_folder, f\"{item_id}_{asset_info.actual_asset_key}{extension}\")\n",
    "                            if os.path.exists(expected_file):\n",
    "                                self.csv_updater.update_status(asset_info.df_index, asset_info.asset_key, \"Downloaded\")\n",
    "                    print(f\"[Download Complete] All assets saved for {item_id}\")\n",
    "                self.activation_manager.completed_items.add(item_id)\n",
    "                return bool(successful_downloads)\n",
    "            finally:\n",
    "                try: os.unlink(temp_zip_path)\n",
    "                except: pass\n",
    "        except Exception as e:\n",
    "            print(f\"[Download Error] {item_id}: {e}\")\n",
    "            return False\n",
    "\n",
    "# =============================================================================\n",
    "# THREAD CONTROLLER\n",
    "# =============================================================================\n",
    "class ThreadedDownloadProcessor:\n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.csv_updater = ThreadSafeCSVUpdater(csv_path)\n",
    "        self.activation_manager = ActivationManager(self.csv_updater)\n",
    "        self.download_manager = DownloadManager(self.csv_updater, self.activation_manager)\n",
    "        self.shutdown_event = threading.Event()\n",
    "        \n",
    "    def process_downloads(self):\n",
    "        print(f\"=== Starting download processing: {self.csv_path} ===\")\n",
    "        try:\n",
    "            self._phase_1_request_activations()\n",
    "            self._phase_2_monitor_and_download()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"[Interrupted] Shutting down...\")\n",
    "            self.shutdown_event.set()\n",
    "    \n",
    "    def _phase_1_request_activations(self):\n",
    "        print(\"Phase 1: Requesting Activations\")\n",
    "        items_to_process = self._get_items_needing_processing()\n",
    "        assets_to_activate = []\n",
    "        for idx, row in items_to_process:\n",
    "            item_id, item_type = row['item_id'], row['item_type']\n",
    "            try:\n",
    "                assets_url = f\"{BASE_URL}/item-types/{item_type}/items/{item_id}/assets/\"\n",
    "                response = session.request('GET', assets_url)\n",
    "                response.raise_for_status()\n",
    "                assets_data = response.json()\n",
    "                for asset_key in ASSET_KEYS:\n",
    "                    current_status = str(row.get(f'{asset_key}_status', ''))\n",
    "                    if \"Downloaded\" in current_status:\n",
    "                        continue\n",
    "                    actual_asset_key = None\n",
    "                    if asset_key in assets_data:\n",
    "                        actual_asset_key = asset_key\n",
    "                    elif ASSET_CONFIG[asset_key]['fallback'] in assets_data:\n",
    "                        actual_asset_key = ASSET_CONFIG[asset_key]['fallback']\n",
    "                    if actual_asset_key:\n",
    "                        assets_to_activate.append(AssetInfo(\n",
    "                            item_id, item_type, asset_key, actual_asset_key, idx, row\n",
    "                        ))\n",
    "            except Exception as e:\n",
    "                print(f\"[Phase1 Error] {item_id}: {e}\")\n",
    "        print(f\"Starting activation for {len(assets_to_activate)} assets\")\n",
    "        with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_ACTIVATIONS) as executor:\n",
    "            futures = [executor.submit(self.activation_manager.request_activation, asset) for asset in assets_to_activate]\n",
    "            for i, _ in enumerate(as_completed(futures), 1):\n",
    "                if i % 20 == 0:\n",
    "                    print(f\"  {i}/{len(assets_to_activate)} activation requests done...\")\n",
    "    \n",
    "    def _phase_2_monitor_and_download(self):\n",
    "        print(\"Phase 2: Monitoring Activations + Downloads\")\n",
    "        start_time = datetime.now()\n",
    "        last_check = datetime.now()\n",
    "        download_futures = []\n",
    "        executor = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_DOWNLOADS)\n",
    "        try:\n",
    "            while not self.shutdown_event.is_set():\n",
    "                if (datetime.now() - start_time).total_seconds() > MAX_ACTIVATION_WAIT:\n",
    "                    print(\"Max activation wait time reached.\")\n",
    "                    break\n",
    "                if (datetime.now() - last_check).total_seconds() >= ACTIVATION_CHECK_INTERVAL:\n",
    "                    self.activation_manager.check_activations()\n",
    "                    last_check = datetime.now()\n",
    "                    status = self.activation_manager.get_activation_status()\n",
    "                    print(f\"Status Update: {status['active_count']} activating, \"\n",
    "                          f\"{status['ready_count']} ready, \"\n",
    "                          f\"{status['download_queue_size']} queued, \"\n",
    "                          f\"{status['completed_count']} completed, \"\n",
    "                          f\"{status['failed_count']} failed, \"\n",
    "                          f\"{len(download_futures)} downloading\")\n",
    "                while True:\n",
    "                    item_group = self.download_manager.get_next_download_group()\n",
    "                    if not item_group: break\n",
    "                    future = executor.submit(self.download_manager.download_item_assets, item_group)\n",
    "                    download_futures.append(future)\n",
    "                download_futures = [f for f in download_futures if not f.done()]\n",
    "                status = self.activation_manager.get_activation_status()\n",
    "                if (status['active_count']==0 and status['ready_count']==0 \n",
    "                    and status['download_queue_size']==0 and not download_futures):\n",
    "                    print(\"[Phase2] All processing complete.\")\n",
    "                    break\n",
    "                time.sleep(30)\n",
    "        finally:\n",
    "            executor.shutdown(wait=True)\n",
    "\n",
    "    def _get_items_needing_processing(self) -> List[Tuple[int, pd.Series]]:\n",
    "        items = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            for asset_key in ASSET_KEYS:\n",
    "                status = str(row.get(f\"{asset_key}_status\", \"\"))\n",
    "                if any(x in status for x in [\"Needs\", \"Failed\", \"Timeout\"]) and \"Downloaded\" not in status:\n",
    "                    items.append((idx, row)); break\n",
    "        return items\n",
    "# =============================================================================\n",
    "# CSV GENERATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def scan_existing_downloads(study_folder: str) -> Dict[str, Dict[str, bool]]:\n",
    "    \"\"\"Scan for existing downloads and return asset availability per item\"\"\"\n",
    "    print(f\"Scanning existing downloads in: {study_folder}\")\n",
    "    \n",
    "    downloaded_items = {}\n",
    "    \n",
    "    if not os.path.exists(study_folder):\n",
    "        print(f\"  Study folder does not exist: {study_folder}\")\n",
    "        return downloaded_items\n",
    "    \n",
    "    # Get all files recursively\n",
    "    for root, dirs, files in os.walk(study_folder):\n",
    "        for file in files:\n",
    "            # Try to match Planet file patterns\n",
    "            for asset_key, config in ASSET_CONFIG.items():\n",
    "                ext = config['extension']\n",
    "                if file.endswith(ext):\n",
    "                    # Try both 4b and 3b versions\n",
    "                    for suffix in [asset_key, config['fallback']]:\n",
    "                        if suffix in file:\n",
    "                            # Extract item_id\n",
    "                            item_id = extract_item_id_from_filename(file, suffix, ext)\n",
    "                            if item_id:\n",
    "                                if item_id not in downloaded_items:\n",
    "                                    downloaded_items[item_id] = {key: False for key in ASSET_KEYS}\n",
    "                                downloaded_items[item_id][asset_key] = True\n",
    "                                break\n",
    "    \n",
    "    print(f\"Found {len(downloaded_items)} items with existing downloads\")\n",
    "    return downloaded_items\n",
    "\n",
    "def extract_item_id_from_filename(filename: str, asset_suffix: str, extension: str) -> Optional[str]:\n",
    "    \"\"\"Extract Planet item_id from filename\"\"\"\n",
    "    # Remove extension\n",
    "    base = filename.replace(extension, '')\n",
    "    \n",
    "    # Remove asset suffix\n",
    "    if base.endswith(f'_{asset_suffix}'):\n",
    "        item_id = base[:-len(f'_{asset_suffix}')]\n",
    "    elif base.endswith(asset_suffix):\n",
    "        item_id = base[:-len(asset_suffix)]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Validate item_id format (Planet IDs are typically YYYYMMDD_HHMMSS_XXXX)\n",
    "    parts = item_id.split('_')\n",
    "    if len(parts) >= 3 and len(parts[0]) == 8 and len(parts[1]) == 6:\n",
    "        return item_id\n",
    "    \n",
    "    return None\n",
    "\n",
    "def search_images(aoi: dict, start_date: str, end_date: str) -> List[dict]:\n",
    "    \"\"\"Search for Planet images with full pagination\"\"\"\n",
    "    print(f\"Searching images from {start_date} to {end_date}\")\n",
    "    \n",
    "    search_request = {\n",
    "        \"item_types\": [\"PSScene\"],\n",
    "        \"filter\": {\n",
    "            \"type\": \"AndFilter\",\n",
    "            \"config\": [\n",
    "                {\n",
    "                    \"type\": \"GeometryFilter\",\n",
    "                    \"field_name\": \"geometry\",\n",
    "                    \"config\": aoi\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"DateRangeFilter\",\n",
    "                    \"field_name\": \"acquired\", \n",
    "                    \"config\": {\n",
    "                        \"gte\": start_date,\n",
    "                        \"lte\": end_date\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"RangeFilter\",\n",
    "                    \"field_name\": \"cloud_cover\",\n",
    "                    \"config\": {\n",
    "                        \"lte\": CLOUD_THRESHOLD\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    all_features = []\n",
    "    url = f\"{BASE_URL}/quick-search\"\n",
    "    is_first = True\n",
    "\n",
    "    while url:\n",
    "        try:\n",
    "            if is_first:\n",
    "                response = session.request('POST', url, json=search_request)\n",
    "                is_first = False\n",
    "            else:\n",
    "                response = session.request('GET', url)\n",
    "                \n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            features = data.get('features', [])\n",
    "            all_features.extend(features)\n",
    "            \n",
    "            print(f\"  Retrieved {len(features)} images (total: {len(all_features)})\")\n",
    "            \n",
    "            url = data.get('_links', {}).get('_next')\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Total images found: {len(all_features)}\")\n",
    "    return all_features\n",
    "\n",
    "def check_asset_availability(item_id: str, item_type: str) -> Tuple[bool, Dict[str, str]]:\n",
    "    \"\"\"Check if all required assets exist (with 3b/4b fallback)\"\"\"\n",
    "    url = f\"{BASE_URL}/item-types/{item_type}/items/{item_id}/assets/\"\n",
    "    \n",
    "    try:\n",
    "        response = session.request('GET', url)\n",
    "        response.raise_for_status()\n",
    "        assets_data = response.json()\n",
    "        \n",
    "        asset_versions = {}\n",
    "        all_exist = True\n",
    "        \n",
    "        for asset_key, config in ASSET_CONFIG.items():\n",
    "            # Try 4b version first\n",
    "            if asset_key in assets_data:\n",
    "                asset_versions[asset_key] = \"4b\"\n",
    "            # Try 3b fallback\n",
    "            elif config['fallback'] in assets_data:\n",
    "                asset_versions[asset_key] = \"3b\"\n",
    "            else:\n",
    "                all_exist = False\n",
    "                break\n",
    "        \n",
    "        return all_exist, asset_versions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking assets for {item_id}: {e}\")\n",
    "        return False, {}\n",
    "\n",
    "def generate_download_csv() -> str:\n",
    "    \"\"\"Generate CSV for the currently configured study area\"\"\"\n",
    "    if not all([CURRENT_STUDY_AREA, CURRENT_AOI, CURRENT_SEASONS, CURRENT_YEARS, CURRENT_FOLDER]):\n",
    "        raise ValueError(\"Must set all configuration variables first!\")\n",
    "    \n",
    "    print(f\"\\nGenerating CSV for {CURRENT_STUDY_AREA}\")\n",
    "    \n",
    "    # Get existing downloads\n",
    "    existing_downloads = scan_existing_downloads(CURRENT_FOLDER)\n",
    "    \n",
    "    # Generate filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = f\"planet_download_queue_{timestamp}.csv\"\n",
    "    csv_path = os.path.join(CURRENT_FOLDER, csv_filename)\n",
    "    \n",
    "    # Ensure folder exists\n",
    "    os.makedirs(CURRENT_FOLDER, exist_ok=True)\n",
    "    \n",
    "    # Prepare CSV headers\n",
    "    headers = ['study_area', 'year', 'season', 'item_id', 'item_type', 'acquired', 'cloud_cover', 'assets_url']\n",
    "    for asset_key in ASSET_KEYS:\n",
    "        headers.append(f'{asset_key}_status')\n",
    "    \n",
    "    records = []\n",
    "    total_checked = 0\n",
    "    items_added = 0\n",
    "\n",
    "    # Summary tracking\n",
    "    summary_by_year = {}\n",
    "    asset_needs = {asset: 0 for asset in ASSET_KEYS}\n",
    "    \n",
    "    # Process each year and season\n",
    "    for year in CURRENT_YEARS:\n",
    "        print(f\"  Processing year {year}\")\n",
    "        \n",
    "        for season_name, (start_template, end_template) in CURRENT_SEASONS.items():\n",
    "            start_date = start_template.format(year=year)\n",
    "            end_date = end_template.format(year=year)\n",
    "            \n",
    "            print(f\"    {season_name}: {start_date} to {end_date}\")\n",
    "            \n",
    "            # Search for images\n",
    "            features = search_images(CURRENT_AOI, start_date, end_date)\n",
    "            \n",
    "            for feature in features:\n",
    "                total_checked += 1\n",
    "                item_id = feature['id']\n",
    "                item_type = feature['properties']['item_type']\n",
    "                acquired = feature['properties']['acquired']\n",
    "                cloud_cover = feature['properties']['cloud_cover']\n",
    "                \n",
    "                # Create year summary record if needed\n",
    "                if year not in summary_by_year:\n",
    "                    summary_by_year[year] = {\n",
    "                        \"Image Sets Found\": 0,\n",
    "                        \"Excluded (Missing Assets)\": 0,\n",
    "                        \"Fully Downloaded\": 0,\n",
    "                        \"Needs Download\": 0\n",
    "                    }\n",
    "                \n",
    "                summary_by_year[year][\"Image Sets Found\"] += 1\n",
    "\n",
    "                # Check asset availability\n",
    "                all_exist, asset_versions = check_asset_availability(item_id, item_type)\n",
    "                if not all_exist:\n",
    "                    summary_by_year[year][\"Excluded (Missing Assets)\"] += 1\n",
    "                    continue\n",
    "\n",
    "                # Create record\n",
    "                assets_url = f\"{BASE_URL}/item-types/{item_type}/items/{item_id}/assets/\"\n",
    "                record = [CURRENT_STUDY_AREA, year, season_name, item_id, item_type, acquired, cloud_cover, assets_url]\n",
    "                \n",
    "                # Add asset status\n",
    "                existing = existing_downloads.get(item_id, {})\n",
    "                asset_statuses = []\n",
    "                all_downloaded = True\n",
    "                \n",
    "                for asset_key in ASSET_KEYS:\n",
    "                    if existing.get(asset_key, False):\n",
    "                        status = \"Downloaded\"\n",
    "                    else:\n",
    "                        version = asset_versions.get(asset_key, \"4b\")\n",
    "                        status = f\"Needs Activation {version.upper()}\"\n",
    "                        all_downloaded = False\n",
    "                        asset_needs[asset_key] += 1\n",
    "                    asset_statuses.append(status)\n",
    "                \n",
    "                record.extend(asset_statuses)\n",
    "                records.append(record)\n",
    "                items_added += 1\n",
    "                \n",
    "                # Update summary\n",
    "                if all_downloaded:\n",
    "                    summary_by_year[year][\"Fully Downloaded\"] += 1\n",
    "                else:\n",
    "                    summary_by_year[year][\"Needs Download\"] += 1\n",
    "                \n",
    "                if total_checked % 100 == 0:\n",
    "                    print(f\"      Processed {total_checked} items, added {items_added}\")\n",
    "    \n",
    "    # Write CSV\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(records)\n",
    "    \n",
    "    # Print Summary by Year\n",
    "    print(f\"\\nCSV saved: {csv_path}\")\n",
    "    print(f\"Total items checked: {total_checked}\")\n",
    "    print(f\"Items added to CSV: {items_added}\")\n",
    "\n",
    "    print(\"\\nImage Set Summary by Year:\")\n",
    "    print(\"{:<8} {:>10} {:>25} {:>18} {:>17}\".format(\"Year\", \"Found\", \"Excluded (Missing Assets)\", \"Downloaded\", \"Needs Download\"))\n",
    "    for year, stats in sorted(summary_by_year.items()):\n",
    "        print(\"{:<8} {:>10} {:>25} {:>18} {:>17}\".format(\n",
    "            year,\n",
    "            stats[\"Image Sets Found\"],\n",
    "            stats[\"Excluded (Missing Assets)\"],\n",
    "            stats[\"Fully Downloaded\"],\n",
    "            stats[\"Needs Download\"]\n",
    "        ))\n",
    "\n",
    "    # Print per-asset download counts\n",
    "    print(\"\\nPer-Asset Download Needs:\")\n",
    "    print(\"{:<30} {:>10}\".format(\"Asset Type\", \"Needs Download\"))\n",
    "    for asset_key, count in asset_needs.items():\n",
    "        print(\"{:<30} {:>10}\".format(asset_key, count))\n",
    "    \n",
    "    return csv_path\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def process_downloads_from_csv(csv_path: str = None):\n",
    "    \"\"\"Process downloads from a specific CSV file using threading\"\"\"\n",
    "    if csv_path is None:\n",
    "        # Look for CSV in current folder\n",
    "        if not CURRENT_FOLDER:\n",
    "            raise ValueError(\"Must set CURRENT_FOLDER or provide csv_path\")\n",
    "        \n",
    "        csv_pattern = os.path.join(CURRENT_FOLDER, \"planet_download_queue_*.csv\")\n",
    "        csv_files = glob.glob(csv_pattern)\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No CSV files found in {CURRENT_FOLDER}\")\n",
    "            return\n",
    "        \n",
    "        csv_path = max(csv_files, key=os.path.getctime)  # Most recent\n",
    "        print(f\"Using most recent CSV: {os.path.basename(csv_path)}\")\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"CSV file not found: {csv_path}\")\n",
    "        return\n",
    "    \n",
    "    processor = ThreadedDownloadProcessor(csv_path)\n",
    "    processor.process_downloads()\n",
    "\n",
    "def process_all_downloads():\n",
    "    \"\"\"Process downloads from all CSV files\"\"\"\n",
    "    if not CURRENT_FOLDER:\n",
    "        raise ValueError(\"Must set CURRENT_FOLDER first\")\n",
    "    \n",
    "    csv_pattern = os.path.join(CURRENT_FOLDER, \"planet_download_queue_*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found. Run generate_download_csv() first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process:\")\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"  {os.path.basename(csv_file)}\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing: {os.path.basename(csv_file)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            process_downloads_from_csv(csv_file)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"User interrupted processing\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4b8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "global CURRENT_STUDY_AREA, CURRENT_AOI, CURRENT_SEASONS, CURRENT_YEARS, CURRENT_FOLDER\n",
    "\n",
    "# Set primary folder and AOI configuration\n",
    "PRIMARY_FOLDER = r\"E:\\planetscope_lake_ice\\Data\\Input\\YF 50x50 km\"\n",
    "CURRENT_FOLDER = PRIMARY_FOLDER\n",
    "\n",
    "CURRENT_STUDY_AREA = \"YF_50x50\"\n",
    "\n",
    "#YKD BELOW:\n",
    "CURRENT_AOI = { \n",
    "    \"type\": \"Polygon\", \n",
    "    \"coordinates\": [[ \n",
    "        [-146.332784, 66.458245], \n",
    "        [-146.343045, 66.906587], \n",
    "        [-145.201146, 66.906587], \n",
    "        [-145.211407, 66.458245], \n",
    "        [-146.332784, 66.458245], \n",
    "    ]] \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "CURRENT_SEASONS = {\n",
    "    \"Freezeup\": (\"{year}-10-01T00:00:00Z\", \"{year}-11-30T23:59:59Z\")\n",
    "}\n",
    "\n",
    "\n",
    "# Define years to process\n",
    "CURRENT_YEARS = [2019, 2020, 2021, 2022, 2023, 2024, 2025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e003e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Checking existing downloads...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2. Downloading!\n",
      "=== Starting download processing: E:\\planetscope_lake_ice\\Data\\Input\\YF 50x50 km\\planet_download_queue_20251117_144037.csv ===\n",
      "Phase 1: Requesting Activations\n",
      "Starting activation for 0 assets\n",
      "Phase 2: Monitoring Activations + Downloads\n",
      "[Phase2] All processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check what you already have downloaded, make CSV\n",
    "print(\"\\n1. Checking existing downloads...\")\n",
    "csv_path = generate_download_csv()\n",
    "\n",
    "print(\"\\n\"*20)\n",
    "print(\"2. Downloading!\")\n",
    "process_downloads_from_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1bd5f",
   "metadata": {},
   "source": [
    "Test missing links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08ffe19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_failed_downloads(csv_path: str):\n",
    "    \"\"\"\n",
    "    Retry only assets that are marked 'Ready for Download' or previously failed\n",
    "    but not marked 'Downloaded'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    updater = ThreadSafeCSVUpdater(csv_path)\n",
    "    activation_manager = ActivationManager(updater)\n",
    "    download_manager = DownloadManager(updater, activation_manager)\n",
    "    \n",
    "    # Collect assets ready to download but not yet downloaded\n",
    "    download_ready_assets = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        item_type = row['item_type']\n",
    "        assets_to_download = []\n",
    "        for asset_key in ASSET_KEYS:\n",
    "            status = str(row.get(f\"{asset_key}_status\", \"\"))\n",
    "            if \"Ready for Download\" in status and \"Downloaded\" not in status:\n",
    "                # Create dummy AssetInfo (with only a download URL)\n",
    "                assets_to_download.append(AssetInfo(\n",
    "                    item_id=item_id,\n",
    "                    item_type=item_type,\n",
    "                    asset_key=asset_key,\n",
    "                    actual_asset_key=asset_key,  # direct key\n",
    "                    df_index=idx,\n",
    "                    row=row,\n",
    "                    download_url=row.get('assets_url')  # Will be refreshed below\n",
    "                ))\n",
    "        if assets_to_download:\n",
    "            download_ready_assets[item_id] = assets_to_download\n",
    "\n",
    "    print(f\"Found {len(download_ready_assets)} items ready to re-download\")\n",
    "\n",
    "    # Refresh each assetâ€™s download link and enqueue it\n",
    "    for item_id, assets in download_ready_assets.items():\n",
    "        try:\n",
    "            item_type = assets[0].item_type\n",
    "            assets_url = f\"{BASE_URL}/item-types/{item_type}/items/{item_id}/assets/\"\n",
    "            response = session.request(\"GET\", assets_url)\n",
    "            response.raise_for_status()\n",
    "            assets_data = response.json()\n",
    "            ready_assets = []\n",
    "            for asset_info in assets:\n",
    "                if asset_info.actual_asset_key in assets_data:\n",
    "                    asset_data = assets_data[asset_info.actual_asset_key]\n",
    "                    if asset_data.get('status') == 'active' and 'location' in asset_data:\n",
    "                        asset_info.download_url = asset_data['location']\n",
    "                        ready_assets.append(asset_info)\n",
    "            if ready_assets:\n",
    "                activation_manager.download_queue.put(\n",
    "                    ItemDownloadGroup(item_id, ready_assets, ready_assets[0].df_index, ready_assets[0].row)\n",
    "                )\n",
    "                print(f\"Re-added {item_id} with {len(ready_assets)} assets to download queue.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Retry Error] {item_id}: {e}\")\n",
    "    \n",
    "    # Run threaded download for queued items\n",
    "    print(\"Starting re-download...\")\n",
    "    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_DOWNLOADS) as executor:\n",
    "        futures = []\n",
    "        while True:\n",
    "            item_group = download_manager.get_next_download_group()\n",
    "            if not item_group:\n",
    "                break\n",
    "            futures.append(executor.submit(download_manager.download_item_assets, item_group))\n",
    "        \n",
    "        for f in as_completed(futures):\n",
    "            f.result()\n",
    "    print(\"Retry process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843909b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 items ready to re-download\n",
      "Re-added 20231004_202309_44_24c9 with 1 assets to download queue.\n",
      "Re-added 20231004_211116_48_2446 with 1 assets to download queue.\n",
      "Re-added 20231004_211114_37_2446 with 1 assets to download queue.\n",
      "Re-added 20241025_212938_32_251d with 2 assets to download queue.\n",
      "Re-added 20241025_212701_49_24f0 with 2 assets to download queue.\n",
      "Re-added 20241020_212835_76_24c6 with 2 assets to download queue.\n",
      "Re-added 20241019_204124_51_24a7 with 2 assets to download queue.\n",
      "Re-added 20241018_204200_85_24ce with 2 assets to download queue.\n",
      "Re-added 20241018_204551_84_24c4 with 2 assets to download queue.\n",
      "Re-added 20241015_212457_85_24f4 with 2 assets to download queue.\n",
      "Re-added 20241003_213023_64_24c6 with 1 assets to download queue.\n",
      "Re-added 20241003_213538_24_251d with 1 assets to download queue.\n",
      "Re-added 20241003_213356_52_24df with 2 assets to download queue.\n",
      "Re-added 20241002_213602_58_2512 with 2 assets to download queue.\n",
      "Re-added 20241001_203701_46_24bc with 1 assets to download queue.\n",
      "Re-added 20251012_214356_99_251f with 2 assets to download queue.\n",
      "Re-added 20251011_214619_99_252b with 2 assets to download queue.\n",
      "Starting re-download...\n",
      "[Download Complete] All assets saved for 20231004_211116_48_2446\n",
      "[Download Complete] All assets saved for 20231004_211114_37_2446\n",
      "[Download Complete] All assets saved for 20231004_202309_44_24c9\n",
      "[Download Complete] All assets saved for 20241025_212938_32_251d\n",
      "[Download Complete] All assets saved for 20241025_212701_49_24f0\n",
      "[Download Complete] All assets saved for 20241020_212835_76_24c6\n",
      "[Download Complete] All assets saved for 20241019_204124_51_24a7\n",
      "[Download Complete] All assets saved for 20241018_204551_84_24c4\n",
      "[Download Complete] All assets saved for 20241015_212457_85_24f4\n",
      "[Download Complete] All assets saved for 20241018_204200_85_24ce\n",
      "[Download Complete] All assets saved for 20241003_213023_64_24c6\n",
      "[Download Complete] All assets saved for 20241003_213538_24_251d\n",
      "[Download Complete] All assets saved for 20241001_203701_46_24bc\n",
      "[Download Complete] All assets saved for 20241003_213356_52_24df\n",
      "[Download Complete] All assets saved for 20241002_213602_58_2512\n"
     ]
    }
   ],
   "source": [
    "retry_failed_downloads(csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
