{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import uuid\n",
    "import time\n",
    "from datetime import datetime\n",
    "from osgeo import ogr\n",
    "from shapely.geometry import shape, mapping, Polygon as ShapelyPolygon, MultiPolygon\n",
    "from pyproj import CRS, Transformer\n",
    "from labelbox import Client\n",
    "from labelbox.data.annotation_types import Polygon as LBPolygon, Point, Label\n",
    "from labelbox.data.annotation_types.annotation import ObjectAnnotation\n",
    "from labelbox.data.serialization.ndjson import NDJsonConverter\n",
    "from labelbox.schema.annotation_import import MALPredictionImport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# === User inputs ===\n",
    "shapefile_path = r\"E:\\planetscope_lake_ice\\Data\\Input\\Global Mollweide Grid\\global_grid_50km_filtered.shp\"\n",
    "cell_ids_to_find = [9843, 20655, 11753, 5136, 36290, 1340, 14215, 2915, 41698, 39599, 43394, 43754, 9299, 44651, 8208, 13666, 6141, 53431, 8819, 14049, 53715, 8203, 7109, 40502, 18065, 28506, 5825, 12704, 16147, 181, 12874, 10419, 9646, 8041, 49626, 30759, 16454, 9140, 7263, 5781, 13654, 16676, 50135, 58739]\n",
    "\n",
    "# === Load global grid ===\n",
    "grid = gpd.read_file(shapefile_path)\n",
    "\n",
    "# === Prepare output dictionary ===\n",
    "bounding_boxes = {}\n",
    "\n",
    "for cid in cell_ids_to_find:\n",
    "    cell = grid.loc[grid[\"cell_id\"] == cid]\n",
    "    if cell.empty:\n",
    "        print(f\"⚠️ Cell ID {cid} not found — skipping\")\n",
    "        continue\n",
    "\n",
    "    # Extract geometry in grid CRS (likely Mollweide)\n",
    "    geom_moll = cell.geometry.iloc[0]\n",
    "    coords = list(geom_moll.exterior.coords)\n",
    "\n",
    "    # Put into a GeoDataFrame for coordinate transformation\n",
    "    verts = gpd.GeoDataFrame(geometry=[Polygon(coords)], crs=grid.crs)\n",
    "\n",
    "    # Reproject to EPSG:4326 (lat/lon)\n",
    "    verts_ll = verts.to_crs(epsg=4326)\n",
    "    geom_ll = verts_ll.geometry.iloc[0]\n",
    "\n",
    "    # Extract corner coordinates\n",
    "    latlon_coords = list(geom_ll.exterior.coords)\n",
    "\n",
    "    # Build GeoJSON-style dict\n",
    "    rect = {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [latlon_coords]\n",
    "    }\n",
    "\n",
    "    # Add to dictionary\n",
    "    site_name = f\"Cell_{cid}\"\n",
    "    bounding_boxes[site_name] = rect\n",
    "\n",
    "print(f\"✅ Constructed bounding_boxes for {len(bounding_boxes)} cells\")\n",
    "bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d15f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User-defined variables ---\n",
    "pld_gdb_path = r\"E:\\planetscope_lake_ice\\Data\\Input\\PLD\\SWOT_PLD_v201_02042025_attributes_updated.gdb\"\n",
    "output_root  = r\"E:\\planetscope_lake_ice\\Data\\Input\"\n",
    "config_file  = r\"D:\\planetscope_lake_ice\\labelbox_water_body_delineation_config.yaml\"\n",
    "\n",
    "\"\"\"\n",
    "# If you want to do this manually, input a dictionary of sites and AOI bounding boxes in Lat, Lon coordinates (EPSG 4326)\n",
    "bounding_boxes = {\n",
    "    \"Cell_9843\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [[\n",
    "            [-93.767792, 57.174268],\n",
    "            [-94.500054, 57.174268],\n",
    "            [-95.340362, 57.683786],\n",
    "            [-94.601588, 57.683786],\n",
    "            [-93.767792, 57.174268]\n",
    "        ]]\n",
    "    },\n",
    "}\"\"\"\n",
    "\n",
    "# --- Labelbox setup ---\n",
    "with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "client = Client(cfg[\"api_key\"])\n",
    "project = client.get_project(cfg[\"project_id\"])\n",
    "\n",
    "print(f\"Connected to project '{project.name}' (ID: {project.uid})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53516303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_lakes_within_bbox(pld_gdb_path, rect, study_site, output_root):\n",
    "    ds = ogr.Open(pld_gdb_path, 0)\n",
    "    if ds is None:\n",
    "        raise RuntimeError(f\"Cannot open GDB: {pld_gdb_path}\")\n",
    "    layer = ds.GetLayerByIndex(0)\n",
    "    in_srs = layer.GetSpatialRef()\n",
    "    print(f\"Input CRS: {in_srs.ExportToProj4()}\")\n",
    "\n",
    "    aoi_poly = shape(rect)\n",
    "    minx, miny, maxx, maxy = aoi_poly.bounds\n",
    "    layer.SetSpatialFilterRect(minx, miny, maxx, maxy)\n",
    "\n",
    "    # Output directories\n",
    "    site_base = os.path.join(output_root, f\"{study_site} 50x50 km - PLD\")\n",
    "    shp_dir = os.path.join(site_base, f\"{study_site} Lakes Raw PLD - Shapefile\")\n",
    "    os.makedirs(shp_dir, exist_ok=True)\n",
    "    shp_path = os.path.join(shp_dir, f\"{study_site}_50x50km_lakes.shp\")\n",
    "\n",
    "    # Write shapefile\n",
    "    driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    if os.path.exists(shp_path):\n",
    "        driver.DeleteDataSource(shp_path)\n",
    "    out_ds = driver.CreateDataSource(shp_path)\n",
    "    out_layer = out_ds.CreateLayer(\"clipped\", srs=in_srs, geom_type=ogr.wkbPolygon)\n",
    "    in_defn = layer.GetLayerDefn()\n",
    "    for i in range(in_defn.GetFieldCount()):\n",
    "        out_layer.CreateField(in_defn.GetFieldDefn(i))\n",
    "\n",
    "    lakes_data, count = [], 0\n",
    "    for feat in layer:\n",
    "        geom = feat.GetGeometryRef()\n",
    "        if geom is None:\n",
    "            continue\n",
    "        shapely_geom = shape(json.loads(geom.ExportToJson()))\n",
    "        if aoi_poly.contains(shapely_geom):\n",
    "            out_feat = ogr.Feature(out_layer.GetLayerDefn())\n",
    "            for i in range(in_defn.GetFieldCount()):\n",
    "                out_feat.SetField(in_defn.GetFieldDefn(i).GetNameRef(), feat.GetField(i))\n",
    "            out_feat.SetGeometry(ogr.CreateGeometryFromJson(json.dumps(mapping(shapely_geom))))\n",
    "            out_layer.CreateFeature(out_feat)\n",
    "            out_feat = None\n",
    "\n",
    "            lake_id = feat.GetField('lake_id') or str(uuid.uuid4())\n",
    "            lakes_data.append({\n",
    "                \"lake_id\": lake_id,\n",
    "                \"geometry\": shapely_geom,\n",
    "                \"properties\": {\n",
    "                    in_defn.GetFieldDefn(i).GetNameRef(): feat.GetField(i)\n",
    "                    for i in range(in_defn.GetFieldCount())\n",
    "                }\n",
    "            })\n",
    "            count += 1\n",
    "    out_ds = None\n",
    "    print(f\"Saved {count} lakes → {shp_path}\")\n",
    "    return shp_path, lakes_data\n",
    "\n",
    "\n",
    "def create_xyz_tile_data_row(client, project, study_site, bbox_geojson):\n",
    "    dataset_name = \"Lake_Ice_Water_Bodies\"\n",
    "    try:\n",
    "        dataset = next(ds for ds in client.get_datasets() if ds.name == dataset_name)\n",
    "    except StopIteration:\n",
    "        raise RuntimeError(f\"Dataset {dataset_name} not found\")\n",
    "\n",
    "    coords = bbox_geojson[\"coordinates\"][0]\n",
    "    lons = [c[0] for c in coords]\n",
    "    lats = [c[1] for c in coords]\n",
    "    bounds = [[min(lats), min(lons)], [max(lats), max(lons)]]\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    external_id = f\"{study_site}_{timestamp}\"\n",
    "\n",
    "    dr_dict = {\n",
    "        \"row_data\": {\n",
    "            \"tile_layer_url\": \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
    "            \"bounds\": bounds,\n",
    "            \"zoom_levels\": {\"min\": 1, \"max\": 18},\n",
    "            \"geometry\": bbox_geojson\n",
    "        },\n",
    "        \"media_type\": \"TMS_GEO\",\n",
    "        \"global_key\": external_id,\n",
    "        \"external_id\": external_id,\n",
    "    }\n",
    "\n",
    "    task = dataset.create_data_rows([dr_dict])\n",
    "    task.wait_till_done()\n",
    "    if task.errors:\n",
    "        raise RuntimeError(\"Upload errors: \" + str(task.errors))\n",
    "    dr_id = task.result[0][\"id\"]\n",
    "\n",
    "    data_row = client.get_data_row(dr_id)\n",
    "    print(f\"Created new DataRow {data_row.uid}\")\n",
    "\n",
    "    project.create_batch(\n",
    "        name=f\"{study_site}_batch_{timestamp}\",\n",
    "        data_rows=[data_row.uid],\n",
    "        priority=1\n",
    "    )\n",
    "\n",
    "    return data_row, dataset\n",
    "\n",
    "\n",
    "def upload_prelabels_with_boundary(client, project, data_row, lakes_data, rect, study_site):\n",
    "    ontology = project.ontology()\n",
    "    polygon_tool = next((t for t in ontology.normalized[\"tools\"] if t[\"name\"].lower() == \"lakes\"), None)\n",
    "    boundary_tool = next((t for t in ontology.normalized[\"tools\"] if t[\"name\"].lower() == \"bounding box\"), None)\n",
    "    if not polygon_tool or not boundary_tool:\n",
    "        raise ValueError(\"Required ontology tools not found (expecting 'Lakes' and 'Bounding box')\")\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    # Add bounding box polygon\n",
    "    boundary_points = [Point(x=lon, y=lat) for lon, lat in rect[\"coordinates\"][0]]\n",
    "    boundary_poly = LBPolygon(points=boundary_points)\n",
    "    boundary_label = Label(\n",
    "        data={\"uid\": data_row.uid},\n",
    "        annotations=[ObjectAnnotation(name=boundary_tool[\"name\"], value=boundary_poly)]\n",
    "    )\n",
    "    labels.append(boundary_label)\n",
    "\n",
    "    # Add each lake polygon\n",
    "    for lake in lakes_data:\n",
    "        geom = lake[\"geometry\"]\n",
    "        if geom.is_empty:\n",
    "            continue\n",
    "        polys = [geom] if geom.geom_type == \"Polygon\" else list(geom.geoms)\n",
    "        for poly in polys:\n",
    "            coords = list(poly.exterior.coords)[:-1]\n",
    "            lb_poly = LBPolygon(points=[Point(x=lon, y=lat) for lon, lat in coords])\n",
    "            ann = ObjectAnnotation(name=polygon_tool[\"name\"], value=lb_poly)\n",
    "            labels.append(Label(data={\"uid\": data_row.uid}, annotations=[ann]))\n",
    "\n",
    "    ndjson = list(NDJsonConverter.serialize(labels))\n",
    "    print(f\"Prepared {len(ndjson)} polygons (boundary + lakes).\")\n",
    "\n",
    "    job = MALPredictionImport.create_from_objects(\n",
    "        client=client,\n",
    "        project_id=project.uid,\n",
    "        name=f\"{study_site}_prelabels_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        predictions=ndjson\n",
    "    )\n",
    "\n",
    "    job.wait_until_done()\n",
    "    if job.errors:\n",
    "        print(\"Errors during upload:\", job.errors)\n",
    "    else:\n",
    "        print(f\"Uploaded boundary and lakes for {study_site}.\")\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79898bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site_name, bbox in bounding_boxes.items():\n",
    "    print(f\"\\n Processing site: {site_name}\")\n",
    "\n",
    "    # Clip and extract lakes for AOI\n",
    "    shp_path, lakes_data = clip_lakes_within_bbox(pld_gdb_path, bbox, site_name, output_root)\n",
    "    print(f\"→ Found {len(lakes_data)} lakes\")\n",
    "\n",
    "    if not lakes_data:\n",
    "        print(\"No lakes found — skipping upload\")\n",
    "        continue\n",
    "\n",
    "    # Create Labelbox TMS DataRow\n",
    "    data_row, dataset = create_xyz_tile_data_row(client, project, site_name, bbox)\n",
    "\n",
    "    # Upload prelabels (lakes + AOI boundary)\n",
    "    job = upload_prelabels_with_boundary(client, project, data_row, lakes_data, bbox, site_name)\n",
    "\n",
    "    print(f\"Finished upload for {site_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
