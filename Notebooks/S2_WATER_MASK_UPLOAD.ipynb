{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348e8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import uuid\n",
    "import time\n",
    "from datetime import datetime\n",
    "from osgeo import ogr\n",
    "from shapely.geometry import shape, mapping, Polygon as ShapelyPolygon, MultiPolygon\n",
    "from pyproj import CRS, Transformer\n",
    "from ipywidgets import Output\n",
    "from labelbox import Client\n",
    "from ipyleaflet import Map, Marker, Polygon as LeafletPolygon, basemaps\n",
    "from labelbox.data.annotation_types import Polygon as LBPolygon, Point, Label\n",
    "from labelbox.data.annotation_types.annotation import ObjectAnnotation\n",
    "from labelbox.schema.ontology import Ontology\n",
    "from labelbox.schema.data_row import DataRow \n",
    "from labelbox.data.serialization.ndjson import NDJsonConverter\n",
    "from labelbox.schema.annotation_import import MALPredictionImport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51066170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e61ba6565943f295f170653bf0ac78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[35.65, 86.45], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_o…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cce80c941049df998fa3864104ebdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Parameters ----\n",
    "square_size_km = 50   # box size (50 x 50 km)\n",
    "initial_location = (35.65, 86.45)  # Starting point - adjust as needed\n",
    "\n",
    "# ---- Helper to compute box corners ----\n",
    "def make_bbox(lat0, lon0, square_size_km=50):\n",
    "    aeqd = CRS.from_proj4(f\"+proj=aeqd +lat_0={lat0} +lon_0={lon0} +units=m +datum=WGS84\")\n",
    "    wgs84 = CRS.from_epsg(4326)\n",
    "    to_aeqd = Transformer.from_crs(wgs84, aeqd, always_xy=True)\n",
    "    to_wgs84 = Transformer.from_crs(aeqd, wgs84, always_xy=True)\n",
    "    half_side = (square_size_km * 1000) / 2\n",
    "    proj_corners = [(-half_side,-half_side),(-half_side,half_side),(half_side,half_side),\n",
    "                    (half_side,-half_side),(-half_side,-half_side)]\n",
    "    return [to_wgs84.transform(x,y)[::-1] for x,y in proj_corners]\n",
    "\n",
    "# ---- Map + drag marker ----\n",
    "out = Output()\n",
    "clicked_coords = {'lat0': initial_location[0], 'lon0': initial_location[1]}\n",
    "marker = Marker(location=initial_location, draggable=True)\n",
    "\n",
    "# Use the **Leaflet** Polygon for drawing the bounding box\n",
    "bbox_polygon = LeafletPolygon(\n",
    "    locations=make_bbox(*initial_location, square_size_km),\n",
    "    color=\"red\", weight=3, fill=False\n",
    ")\n",
    "\n",
    "m = Map(center=initial_location, zoom=7, basemap=basemaps.Esri.WorldImagery,\n",
    "        scroll_wheel_zoom=True)\n",
    "m.add_layer(marker)\n",
    "m.add_layer(bbox_polygon)\n",
    "\n",
    "def on_drag_end(event=None, **kwargs):\n",
    "    lat, lon = marker.location\n",
    "    clicked_coords['lat0'], clicked_coords['lon0'] = lat, lon\n",
    "    bbox_polygon.locations = make_bbox(lat, lon, square_size_km)\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        print(f\"Selected Center → lat0={lat:.4f}, lon0={lon:.4f}\")\n",
    "\n",
    "marker.observe(on_drag_end, names='location')\n",
    "display(m, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d15f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study site: GR\n",
      "Center: 68.618536, 36.611939\n",
      "Bounding box created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nrect = { \\n\\t\"type\": \"Polygon\", \\n\\t\"coordinates\": [ [ \\n\\t\\t[-51.158238, 67.256290], \\n\\t\\t[-51.169263, 67.704585], \\n\\t\\t[-49.988726, 67.704585], \\n\\t\\t[-49.999751, 67.256290], \\n\\t\\t[-51.158238, 67.256290], \\n\\t] ] \\n}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User-defined variables\n",
    "study_site = \"GR\"  # Your site name\n",
    "pld_gdb_path = r\"E:\\planetscope_lake_ice\\Data\\Input\\PLD\\SWOT_PLD_v201_02042025_attributes_updated.gdb\"\n",
    "output_root = r\"E:\\planetscope_lake_ice\\Data\\Input\"\n",
    "config_file = r\"D:\\planetscope_lake_ice\\labelbox_water_body_delineation_config.yaml\"\n",
    "\n",
    "# Build rect polygon from marker location\n",
    "lat0, lon0 = clicked_coords['lat0'], clicked_coords['lon0']\n",
    "corners = make_bbox(lat0, lon0, square_size_km)\n",
    "\n",
    "# GeoJSON rect with proper (lon,lat) ordering\n",
    "rect = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [[(lon, lat) for lat, lon in corners]]\n",
    "}\n",
    "\n",
    "print(f\"Study site: {study_site}\")\n",
    "print(f\"Center: {lat0:.6f}, {lon0:.6f}\")\n",
    "print(f\"Bounding box created\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "rect = { \n",
    "\t\"type\": \"Polygon\", \n",
    "\t\"coordinates\": [ [ \n",
    "\t\t[-51.158238, 67.256290], \n",
    "\t\t[-51.169263, 67.704585], \n",
    "\t\t[-49.988726, 67.704585], \n",
    "\t\t[-49.999751, 67.256290], \n",
    "\t\t[-51.158238, 67.256290], \n",
    "\t] ] \n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad50ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_lakes_within_bbox(pld_gdb_path, rect, study_site, output_root):\n",
    "    ds = ogr.Open(pld_gdb_path, 0)\n",
    "    if ds is None:\n",
    "        raise RuntimeError(f\"Cannot open GDB: {pld_gdb_path}\")\n",
    "    layer = ds.GetLayerByIndex(0)\n",
    "    in_srs = layer.GetSpatialRef()\n",
    "    print(\"Input CRS:\", in_srs.ExportToProj4())\n",
    "\n",
    "    aoi_poly = shape(rect)\n",
    "    minx, miny, maxx, maxy = aoi_poly.bounds\n",
    "    layer.SetSpatialFilterRect(minx, miny, maxx, maxy)\n",
    "\n",
    "    # Output dirs\n",
    "    site_base = os.path.join(output_root, f\"{study_site} 50x50 km - PLD\")\n",
    "    shp_dir = os.path.join(site_base, f\"{study_site} Lakes Raw PLD - Shapefile\")\n",
    "    os.makedirs(shp_dir, exist_ok=True)\n",
    "\n",
    "    shp_path = os.path.join(shp_dir, f\"{study_site}_50x50km_lakes.shp\")\n",
    "\n",
    "    # Write shapefile\n",
    "    driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    if os.path.exists(shp_path):\n",
    "        driver.DeleteDataSource(shp_path)\n",
    "    out_ds = driver.CreateDataSource(shp_path)\n",
    "    out_layer = out_ds.CreateLayer(\"clipped\", srs=in_srs, geom_type=ogr.wkbPolygon)\n",
    "\n",
    "    in_defn = layer.GetLayerDefn()\n",
    "    for i in range(in_defn.GetFieldCount()):\n",
    "        out_layer.CreateField(in_defn.GetFieldDefn(i))\n",
    "\n",
    "    lakes_data = []\n",
    "    count = 0\n",
    "    for feat in layer:\n",
    "        geom = feat.GetGeometryRef()\n",
    "        if geom is None:\n",
    "            continue\n",
    "        shapely_geom = shape(json.loads(geom.ExportToJson()))\n",
    "        if aoi_poly.contains(shapely_geom):\n",
    "            out_feat = ogr.Feature(out_layer.GetLayerDefn())\n",
    "            for i in range(in_defn.GetFieldCount()):\n",
    "                out_feat.SetField(in_defn.GetFieldDefn(i).GetNameRef(), feat.GetField(i))\n",
    "            out_feat.SetGeometry(ogr.CreateGeometryFromJson(json.dumps(mapping(shapely_geom))))\n",
    "            out_layer.CreateFeature(out_feat)\n",
    "            out_feat = None\n",
    "            lake_id = feat.GetField('lake_id') or str(uuid.uuid4())\n",
    "            lakes_data.append({\n",
    "                \"lake_id\": lake_id,\n",
    "                \"geometry\": shapely_geom,\n",
    "                \"properties\": {\n",
    "                    in_defn.GetFieldDefn(i).GetNameRef(): feat.GetField(i)\n",
    "                    for i in range(in_defn.GetFieldCount())\n",
    "                }\n",
    "            })\n",
    "            count += 1\n",
    "\n",
    "    out_ds = None\n",
    "    print(f\"Shapefile saved {count} lakes → {shp_path}\")\n",
    "\n",
    "    return shp_path, lakes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91dfc516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input CRS: +proj=longlat +datum=WGS84 +no_defs\n",
      "Shapefile saved 1280 lakes → E:\\planetscope_lake_ice\\Data\\Input\\GR 50x50 km - PLD\\GR Lakes Raw PLD - Shapefile\\GR_50x50km_lakes.shp\n",
      "\n",
      "Total lakes found: 1280\n",
      "First lake ID: 2520565172.0\n"
     ]
    }
   ],
   "source": [
    "# Execute the clipping\n",
    "shp_path, lakes_data = clip_lakes_within_bbox(\n",
    "    pld_gdb_path, rect, study_site, output_root\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal lakes found: {len(lakes_data)}\")\n",
    "if lakes_data:\n",
    "    print(f\"First lake ID: {lakes_data[0]['lake_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f60e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to project Lake Ice Project - Water Body Delineation (ID: cmen9gtep08l007xl1zhp87lk)\n"
     ]
    }
   ],
   "source": [
    "with open(config_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "api_key = cfg[\"api_key\"]\n",
    "project_id = cfg[\"project_id\"]\n",
    "\n",
    "client = Client(api_key)\n",
    "project = client.get_project(project_id)\n",
    "print(f\"Connected to project {project.name} (ID: {project.uid})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace7943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xyz_tile_data_row(client, project, study_site, lat0, lon0, bbox_geojson):\n",
    "    # Get dataset by name\n",
    "    dataset_name = \"Lake_Ice_Water_Bodies\"\n",
    "    try:\n",
    "        dataset = next(ds for ds in client.get_datasets() if ds.name == dataset_name)\n",
    "    except StopIteration:\n",
    "        raise RuntimeError(f\"Dataset {dataset_name} not found in your Labelbox account\")\n",
    "\n",
    "    print(\"Using dataset:\", dataset.name, f\"({dataset.uid})\")\n",
    "\n",
    "    # Build bounding box\n",
    "    coords = bbox_geojson[\"coordinates\"][0]\n",
    "    lons = [c[0] for c in coords]\n",
    "    lats = [c[1] for c in coords]\n",
    "    lat_min, lat_max = min(lats), max(lats)\n",
    "    lon_min, lon_max = min(lons), max(lons)\n",
    "    bounds = [[lat_min, lon_min], [lat_max, lon_max]]\n",
    "\n",
    "    # Unique ID for DataRow\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    external_id = f\"{study_site}_{timestamp}\"\n",
    "\n",
    "    row_data = {\n",
    "        \"tile_layer_url\": \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
    "        \"bounds\": bounds,\n",
    "        \"zoom_levels\": {\"min\": 1, \"max\": 18},\n",
    "        \"geometry\": bbox_geojson\n",
    "    }\n",
    "\n",
    "    dr_dict = {\n",
    "        \"row_data\": row_data,\n",
    "        \"media_type\": \"TMS_GEO\",\n",
    "        \"global_key\": external_id,\n",
    "        \"external_id\": external_id,\n",
    "    }\n",
    "\n",
    "    task = dataset.create_data_rows([dr_dict])\n",
    "    task.wait_till_done()\n",
    "    if task.errors:\n",
    "        raise RuntimeError(\"Upload errors: \" + str(task.errors))\n",
    "\n",
    "    dr_id = task.result[0][\"id\"]\n",
    "    data_row = client.get_data_row(dr_id)\n",
    "    print(\"Created new DataRow:\", data_row.uid)\n",
    "\n",
    "    # Attach DataRow to the project once\n",
    "    batch_name = f\"{study_site}_batch_{timestamp}\"\n",
    "    project.create_batch(\n",
    "        name=batch_name,\n",
    "        data_rows=[data_row.uid],\n",
    "        priority=1\n",
    "    )\n",
    "    print(f\"Attached DataRow to project via {batch_name}\")\n",
    "\n",
    "    return data_row, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af548813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: Lake_Ice_Water_Bodies (cmen9im6u005q0703rrwm5ymu)\n",
      "Created new DataRow: cmfmti8h94xpf0744orb7xjhz\n",
      "Attached DataRow to project via GR_batch_20250916_131918\n",
      "Using tool: Lakes\n",
      "Prepared 1280 prelabels for upload. Preview:\n",
      "{\n",
      "  \"uuid\": \"0f05a105-1561-488b-ae45-6577caa97991\",\n",
      "  \"dataRow\": {\n",
      "    \"id\": \"cmfmti8h94xpf0744orb7xjhz\"\n",
      "  },\n",
      "  \"name\": \"Lakes\",\n",
      "  \"classifications\": [],\n",
      "  \"polygon\": [\n",
      "    {\n",
      "      \"x\": 36.56249888900004,\n",
      "      \"y\": 68.69915081100004\n",
      "    },\n",
      "    {\n",
      "      \"x\": 36.56102021800007,\n",
      "      \"y\": 68.69912948300004\n",
      "    },\n",
      "    {\n",
      "      \"x\": 36.560697435000066,\n",
      "      \"y\": 68.70208642800003\n",
      "    },\n",
      "    {\n",
      "      \"x ...\n",
      "Uploaded prelabels: GR_prelabels_20250916_131931 to DataRow: cmfmti8h94xpf0744orb7xjhz\n"
     ]
    }
   ],
   "source": [
    "def convert_lakes_to_labelbox_annotations(lakes_data, data_row_id, study_site):\n",
    "    \"\"\"Convert lake geometries to Labelbox annotation format\"\"\"\n",
    "    \n",
    "    annotations = []\n",
    "    \n",
    "    for lake in lakes_data:\n",
    "        lake_id = lake['lake_id']\n",
    "        geom = lake['geometry']\n",
    "        \n",
    "        # Handle different geometry types\n",
    "        polygons_to_process = []\n",
    "        if isinstance(geom, ShapelyPolygon):\n",
    "            polygons_to_process = [geom]\n",
    "        elif isinstance(geom, MultiPolygon):\n",
    "            polygons_to_process = list(geom.geoms)\n",
    "        else:\n",
    "            print(f\"Skipping non-polygon geometry for lake {lake_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Create annotation for each polygon\n",
    "        for poly_idx, poly in enumerate(polygons_to_process):\n",
    "            # Get exterior coordinates (Labelbox expects them in lon,lat order)\n",
    "            exterior_coords = list(poly.exterior.coords[:-1])  # Remove duplicate last point\n",
    "            \n",
    "            # Create Labelbox polygon points\n",
    "            points = []\n",
    "            for lon, lat in exterior_coords:\n",
    "                points.append(Point(x=lon, y=lat))\n",
    "            \n",
    "            # Create the polygon geometry\n",
    "            lb_polygon = LBPolygon(points=points)\n",
    "            \n",
    "            # Create annotation with schema reference\n",
    "            # You'll need to update the schema_id with your actual tool ID\n",
    "            annotation = {\n",
    "                \"uuid\": str(uuid.uuid4()),\n",
    "                \"schemaId\": None,  # Will be set based on your ontology\n",
    "                \"dataRow\": {\"id\": data_row_id},\n",
    "                \"polygon\": lb_polygon\n",
    "            }\n",
    "            \n",
    "            annotations.append(annotation)\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "def upload_prelabels_mal(client, project, data_row, lakes_data, study_site):\n",
    "    # Ontology lookup\n",
    "    ontology = project.ontology()\n",
    "    polygon_tool = next((t for t in ontology.normalized[\"tools\"] if t[\"tool\"] == \"polygon\"), None)\n",
    "    if not polygon_tool:\n",
    "        raise ValueError(\"Polygon tool not found in ontology\")\n",
    "    print(\"Using tool:\", polygon_tool[\"name\"])\n",
    "\n",
    "    # Collect label objects\n",
    "    labels = []\n",
    "    for lake in lakes_data:\n",
    "        geom = lake[\"geometry\"]\n",
    "        if geom.is_empty:\n",
    "            continue\n",
    "        polys = [geom] if geom.geom_type == \"Polygon\" else list(geom.geoms)\n",
    "        for poly in polys:\n",
    "            coords = list(poly.exterior.coords)[:-1]\n",
    "            lb_poly = LBPolygon(points=[Point(x=lon, y=lat) for lon, lat in coords])\n",
    "            obj_ann = ObjectAnnotation(\n",
    "                name=polygon_tool[\"name\"],\n",
    "                value=lb_poly\n",
    "            )\n",
    "            labels.append(\n",
    "                Label(\n",
    "                    data={\"uid\": data_row.uid}, \n",
    "                    annotations=[obj_ann]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    ndjson = list(NDJsonConverter.serialize(labels))\n",
    "    print(\"Prepared\", len(ndjson), \"prelabels for upload. Preview:\")\n",
    "    print(json.dumps(ndjson[0], indent=2)[:400], \"...\")  # preview first item\n",
    "\n",
    "    # Upload MAL prelabels\n",
    "    unique_name = f\"{study_site}_prelabels_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    job = MALPredictionImport.create_from_objects(\n",
    "        client=client,\n",
    "        project_id=project.uid,\n",
    "        name=unique_name,\n",
    "        predictions=ndjson\n",
    "    )\n",
    "    job.wait_until_done()\n",
    "    if job.errors:\n",
    "        print(\"Errors:\", job.errors)\n",
    "    else:\n",
    "        print(\"Uploaded prelabels:\", unique_name, \"to DataRow:\", data_row.uid)\n",
    "\n",
    "    return job\n",
    "\n",
    "data_row, dataset = create_xyz_tile_data_row(client, project, study_site, lat0, lon0, rect)\n",
    "job = upload_prelabels_mal(client, project, data_row, lakes_data, study_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7255520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataRow cmfmti8h94xpf0744orb7xjhz is already attached to project Lake Ice Project - Water Body Delineation.\n",
      "You can now start labeling in the Labelbox UI.\n"
     ]
    }
   ],
   "source": [
    "def add_to_labeling_queue(project, data_row):\n",
    "    \"\"\"Check whether the data row is already in the labeling queue\"\"\"\n",
    "    if not data_row:\n",
    "        print(\"No data row available\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Just confirm association\n",
    "        print(f\"DataRow {data_row.uid} is already attached to project {project.name}.\")\n",
    "        print(\"You can now start labeling in the Labelbox UI.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking DataRow status: {e}\")\n",
    "\n",
    "# Add to labeling queue\n",
    "add_to_labeling_queue(project, data_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
