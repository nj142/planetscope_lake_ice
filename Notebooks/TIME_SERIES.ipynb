{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import rasterio\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw, ImageFont  # Keep the original PIL Image import\n",
    "import rasterio.mask\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "from pyproj import Transformer\n",
    "import time\n",
    "import shapely\n",
    "import re\n",
    "import datetime\n",
    "import string\n",
    "import geopandas\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from IPython.display import Image as IPythonImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> JUPYTER NOTEBOOK: </b>\n",
    "Exploring the structure of a NetCDF file w/ variable length arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://s3.amazonaws.com/hr-challenge-images/14507/1476906485-2c93045320-variable-length-arrays.png)\n",
    "\n",
    "For each lake, I have variable length (vlen) arrays of all my ice cover observations. Vlen data type can be constructed from ints, floats, or any other python type.\n",
    "\n",
    "~ ~ ~ ~ ~ ~ ~\n",
    "\n",
    "The NCDump command below is a quick way to output all the variables in a NetCDF file and show the overall file structure.  \n",
    "\n",
    "![alt text](https://miro.medium.com/v2/resize:fit:720/format:webp/1*p_i85nbMqMQAgTZ_4aexhA.png)\n",
    "\n",
    "Reading below, the vlen_ types are just my variable types for vlen arrays\n",
    "\n",
    "The lake dimension is the structure of my dataframe-- each lake in the \"lake\" dimension has a specific ID (I read these in from ALPOD) which lets me quickly add data based on where the ALPOD shapefile intersects with satellite imagery. The \"lake\" dimension actually was initiated with unlimited size, but I currently have 43929 entries for individual lakes calculated from my time series data. Usually dimensions are time and lat/lon, but since I wanted\n",
    "my data organized by lake, I structured my dimensions around the ALPOD list IDs instead of usual spatiotemporal axes.\n",
    "\n",
    "The other variables like lake_id, study_site, etc. are just normal variables associated with each lake. NetCDF can have variables of all the normal python types.  You can see these all have size of the (lake) dimension since each lake has each of these associated variables. I am working on adding another array variable contraining my red band rasters to the netcdf, so that each lake ID also contains vlen arrays of all the actual Surface Reflectance pixel observations recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Run ncdump command and capture the output\n",
    "output = !ncdump -h \"D:/planetscope_lake_ice/Data (Initial Population)/2 - Break Up Time Series Output/lake_statistics_unlimited.nc\"\n",
    "\n",
    "# Join the output lines into a single string\n",
    "output_text = \"\\n\".join(output)\n",
    "\n",
    "# Display the full output as preformatted text\n",
    "display(HTML(f\"<pre>{output_text}</pre>\"))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your NetCDF fileopath here\n",
    "netcdf_path = r\"D:\\planetscope_lake_ice\\Data (CDF Testing)\\Output\\106.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This script organizes the dataset by # of observations, and prints out the area for the 20 lakes which have the densest time series.\"\"\"\n",
    "# Open the NetCDF file\n",
    "with nc.Dataset(netcdf_path, 'r') as ncfile:\n",
    "    # Get lake IDs and observation counts\n",
    "    lake_ids = ncfile.variables['lake_id'][:]\n",
    "    observation_counts = ncfile.variables['obs_count'][:]\n",
    "    areas = ncfile.variables['area'][:]\n",
    "    study_sites = ncfile.variables['study_site'][:]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Lake ID': lake_ids,\n",
    "    'Number of Observations': observation_counts,\n",
    "    'Area': areas,\n",
    "    'Study Site': study_sites\n",
    "})\n",
    "\n",
    "# Sort by number of observations in descending order\n",
    "top_lakes = df.sort_values('Number of Observations', ascending=False).head(20)\n",
    "\n",
    "print(\"Top 20 Lakes by Number of Observations:\")\n",
    "print(top_lakes.to_string(index=False))\n",
    "\n",
    "# Calculate some statistics\n",
    "total_lakes = len(lake_ids)\n",
    "total_observations = observation_counts.sum()\n",
    "\n",
    "print(f\"\\nTotal Lakes: {total_lakes}\")\n",
    "print(f\"Total Observations: {total_observations}\")\n",
    "print(f\"Average Observations per Lake: {total_observations/total_lakes:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This block is the function to print all data for a single lake, executed in the cell below.\"\"\"\n",
    "\n",
    "def check_lake_data(netcdf_path, lake_id=None):\n",
    "    \"\"\"\n",
    "    Read and display lake data from a NetCDF file with VLEN data format.  \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    netcdf_path : str\n",
    "        Path to the NetCDF file.\n",
    "    lake_id : int, optional\n",
    "        Specific lake ID to inspect. If None, lists all lake IDs.\n",
    "    \"\"\"\n",
    "    with nc.Dataset(netcdf_path, 'r') as ncfile:\n",
    "        # Get all lake IDs\n",
    "        lake_ids = ncfile.variables['lake_id'][:]\n",
    "        \n",
    "        if lake_id is None:\n",
    "            print(\"Available Lake IDs:\")\n",
    "            print(lake_ids)\n",
    "            return\n",
    "        \n",
    "        # Find the index of the specified lake\n",
    "        matches = np.where(lake_ids == lake_id)[0]\n",
    "        if len(matches) == 0:\n",
    "            print(f\"Lake ID {lake_id} not found in the dataset.\")\n",
    "            return\n",
    "        lake_index = matches[0]\n",
    "        \n",
    "        # Print lake-level metadata\n",
    "        print(f\"\\nLake ID: {lake_id}\")\n",
    "        print(f\"Area: {ncfile.variables['area'][lake_index]:.2f} sq meters\")\n",
    "        # If available, print additional metadata\n",
    "        if 'perimeter' in ncfile.variables:\n",
    "            print(f\"Perimeter: {ncfile.variables['perimeter'][lake_index]:.2f} meters\")\n",
    "        if 'study_site' in ncfile.variables:\n",
    "            # Handle study_site, which might be stored as bytes or string\n",
    "            study_site = ncfile.variables['study_site'][lake_index]\n",
    "            if isinstance(study_site, bytes):\n",
    "                study_site = study_site.decode('utf-8')\n",
    "            print(f\"Study Site: {study_site}\")\n",
    "        print(f\"Total Pixels: {ncfile.variables['total_pixels'][lake_index]}\")\n",
    "        \n",
    "        # Use obs_count for the number of observations\n",
    "        obs_count = ncfile.variables['obs_count'][lake_index]\n",
    "        print(f\"Number of Observations: {obs_count}\")\n",
    "        \n",
    "        if obs_count == 0:\n",
    "            print(\"No observations found for this lake.\")\n",
    "            return\n",
    "        \n",
    "        # For VLEN arrays, each element is an array of varying length\n",
    "        # We need to explicitly get each VLEN array for the current lake\n",
    "        \n",
    "        # Build the data dictionary for observations\n",
    "        data = {}\n",
    "        \n",
    "        # For 'unix_time' (stored as Unix timestamps)\n",
    "        timestamps = ncfile.variables['unix_time'][lake_index]\n",
    "        data['Date (Unix)'] = timestamps\n",
    "        \n",
    "        # For 'prefix': it's likely a single string, not a comma-separated list\n",
    "        prefix = ncfile.variables['image_names_csv'][lake_index]\n",
    "        if isinstance(prefix, bytes):\n",
    "            prefix = prefix.decode('utf-8')\n",
    "        \n",
    "        # For all other numeric observation variables with VLEN format\n",
    "        for var_name in ['usable_pixels', 'clear_percent', 'ice_pixels', \n",
    "                         'ice_percent', 'snow_pixels', 'snow_percent', \n",
    "                         'water_pixels', 'water_percent']:\n",
    "            if var_name in ncfile.variables:\n",
    "                data[var_name] = ncfile.variables[var_name][lake_index]\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Add human-readable datetime to the end\n",
    "        df['unix_time'] = df['Date (Unix)'].apply(\n",
    "            lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        )\n",
    "        \n",
    "        print(\"\\nOBSERVATIONS ({}):\".format(obs_count))\n",
    "        print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR LAKE NUMBER HERE\n",
    "lake_id = 197626\n",
    "\n",
    "# Runs above script to print out data for the requested lake ID (ID sourced from ALPOD)\n",
    "check_lake_data(netcdf_path, lake_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This block is the function to turn the data above into a time series graph\"\"\"\n",
    "\n",
    "def plot_time_series_for_lake(netcdf_path, lake_id):\n",
    "    \"\"\"\n",
    "    For a given lake_id, read the VLEN NetCDF file, group observations by year,\n",
    "    and plot time series for ice, snow, and water percentages for each year with data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    netcdf_path : str\n",
    "        Path to the NetCDF file.\n",
    "    lake_id : int\n",
    "        The lake ID to plot data for.\n",
    "    \"\"\"\n",
    "    with nc.Dataset(netcdf_path, 'r') as ncfile:\n",
    "        # Find the index of the specified lake\n",
    "        lake_ids = ncfile.variables['lake_id'][:]\n",
    "        matches = np.where(lake_ids == lake_id)[0]\n",
    "        if len(matches) == 0:\n",
    "            print(f\"Lake ID {lake_id} not found in the dataset.\")\n",
    "            return\n",
    "        lake_index = matches[0]\n",
    "        \n",
    "        # Get number of observations for the lake\n",
    "        obs_count = ncfile.variables['obs_count'][lake_index]\n",
    "        if obs_count == 0:\n",
    "            print(f\"No observations found for lake ID {lake_id}.\")\n",
    "            return\n",
    "\n",
    "        # Extract Unix timestamps and convert to datetime objects\n",
    "        # (Assumes that the variable 'unix_time' is stored as a VLEN array of Unix timestamps)\n",
    "        time_data = ncfile.variables['unix_time'][lake_index]\n",
    "        datetimes = [datetime.datetime.fromtimestamp(ts) for ts in time_data]\n",
    "        \n",
    "        # Extract percentage variables for ice, snow, and water.\n",
    "        # These variables are expected to be arrays of the same length as time_data.\n",
    "        ice_percent = ncfile.variables['ice_percent'][lake_index]\n",
    "        snow_percent = ncfile.variables['snow_percent'][lake_index]\n",
    "        water_percent = ncfile.variables['water_percent'][lake_index]\n",
    "        \n",
    "        # Create a DataFrame with the data and add a year column\n",
    "        df = pd.DataFrame({\n",
    "            'datetime': datetimes,\n",
    "            'ice_percent': ice_percent,\n",
    "            'snow_percent': snow_percent,\n",
    "            'water_percent': water_percent\n",
    "        })\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "\n",
    "    # Define colors for the plots\n",
    "    ice_color   = '#87CEEB'\n",
    "    snow_color  = '#FF8C00'\n",
    "    water_color = '#0000FF'\n",
    "\n",
    "    # Get unique years with observations\n",
    "    unique_years = sorted(df['year'].unique())\n",
    "    \n",
    "    # Loop over each year and plot the time series if data exists for that year\n",
    "    for year in unique_years:\n",
    "        df_year = df[df['year'] == year]\n",
    "        if df_year.empty:\n",
    "            continue\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df_year['datetime'], df_year['ice_percent'], label='Ice %', \n",
    "                 color=ice_color, marker='o')\n",
    "        plt.plot(df_year['datetime'], df_year['snow_percent'], label='Snow %', \n",
    "                 color=snow_color, marker='o')\n",
    "        plt.plot(df_year['datetime'], df_year['water_percent'], label='Water %', \n",
    "                 color=water_color, marker='o')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.title(f\"Lake {lake_id} - Time Series for Year {year}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the time series for the given lake ID above, organized by year\n",
    "plot_time_series_for_lake(netcdf_path, lake_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The functions below all work together to make a GIF of the time series for a given lake ID,\n",
    "constructed from all the available images.  The GIF is output to a given folder and also shown in jupyter.\"\"\"\n",
    "\n",
    "def find_image_paths(base_dir, prefixes):\n",
    "    \"\"\"Find full paths for image files matching given prefixes.\"\"\"\n",
    "    \n",
    "    matching_images = []\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        search_pattern = os.path.join(base_dir, '**', f'{prefix}.tif')\n",
    "        matches = glob.glob(search_pattern, recursive=True)\n",
    "        \n",
    "        if matches:\n",
    "            matching_images.extend(matches)\n",
    "        else:\n",
    "            print(f\"Warning: No image found for prefix {prefix}\")\n",
    "    \n",
    "    return sorted(matching_images)\n",
    "\n",
    "def get_lake_polygon(shapefile_path, lake_id):\n",
    "    \"\"\"Retrieve the polygon and CRS for a specific lake from ALPOD shapefile.\"\"\"\n",
    "    \n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    lake_row = gdf[gdf['id'] == lake_id]\n",
    "    \n",
    "    if len(lake_row) == 0:\n",
    "        print(f\"No lake found with ID {lake_id}\")\n",
    "        return None, None\n",
    "    \n",
    "    return lake_row.geometry.iloc[0], lake_row.crs\n",
    "\n",
    "def create_lake_bounds(lake_polygon, polygon_crs, src_crs):\n",
    "    \"\"\"\n",
    "    Create a bounding box for the lake, transformed to the source CRS.\n",
    "    \n",
    "    Args:\n",
    "        lake_polygon (shapely.geometry.Polygon): Lake polygon\n",
    "        polygon_crs (pyproj.CRS): CRS of the original polygon\n",
    "        src_crs (pyproj.CRS): Target CRS to transform to\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Bounding box coordinates (left, bottom, right, top)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create transformer\n",
    "    transformer = Transformer.from_crs(polygon_crs, src_crs, always_xy=True)\n",
    "    \n",
    "    # Transform polygon coordinates\n",
    "    transformed_coords = []\n",
    "    for x, y in lake_polygon.exterior.coords:\n",
    "        transformed_x, transformed_y = transformer.transform(x, y)\n",
    "        transformed_coords.append((transformed_x, transformed_y))\n",
    "    \n",
    "    # Create a new polygon with transformed coordinates\n",
    "    transformed_polygon = gpd.GeoSeries([shapely.geometry.Polygon(transformed_coords)], crs=src_crs)[0]\n",
    "    \n",
    "    # Get the geographic bounds\n",
    "    left, bottom, right, top = transformed_polygon.bounds\n",
    "    \n",
    "    # Calculate the current width and height\n",
    "    width = right - left\n",
    "    height = top - bottom\n",
    "    \n",
    "    # Add 500m to sides \n",
    "    left -= 500\n",
    "    right += 500\n",
    "    bottom -= 500\n",
    "    top += 500\n",
    "    \n",
    "    # Make it a square by taking the larger dimension\n",
    "    square_size = max(right - left, top - bottom)\n",
    "    \n",
    "    # Center the square on the original bounds\n",
    "    center_x = (left + right) / 2\n",
    "    center_y = (bottom + top) / 2\n",
    "    \n",
    "    half_square = square_size / 2\n",
    "    \n",
    "    # Adjust bounds to create a square\n",
    "    left = center_x - half_square\n",
    "    right = center_x + half_square\n",
    "    bottom = center_y - half_square\n",
    "    top = center_y + half_square\n",
    "    \n",
    "    return (left, bottom, right, top)\n",
    "\n",
    "def process_image(image_path, lake_bounds, lake_polygon=None, polygon_crs=None, downsample_factor=4, title=None):\n",
    "    \"\"\"\n",
    "    Read, crop, and process a geotiff image with lake polygon overlay.\n",
    "    \"\"\"\n",
    "    \n",
    "    with rasterio.open(image_path) as src:\n",
    "        # Crop to the lake bounds\n",
    "        out_image, out_transform = rasterio.mask.mask(\n",
    "            src, \n",
    "            [box(*lake_bounds)], \n",
    "            crop=True, \n",
    "            nodata=0\n",
    "        )\n",
    "        \n",
    "        # Reorder bands from BGR NIR to RGB\n",
    "        rgb_bands = out_image[[2, 1, 0]]  # Red, Green, Blue\n",
    "        \n",
    "        # Downsample\n",
    "        downsampled_image = rgb_bands[:, ::downsample_factor, ::downsample_factor]\n",
    "        \n",
    "        # Transpose to (height, width, channels)\n",
    "        downsampled_image = np.transpose(downsampled_image, (1, 2, 0))\n",
    "        \n",
    "        # Normalize to 0-255 range\n",
    "        downsampled_image = ((downsampled_image - downsampled_image.min()) / \n",
    "                             (downsampled_image.max() - downsampled_image.min()) * 255).astype(np.uint8)\n",
    "        \n",
    "        # Convert to PIL Image for further processing\n",
    "        pil_image = Image.fromarray(downsampled_image)  # Using PIL's Image directly\n",
    "        draw = ImageDraw.Draw(pil_image, 'RGBA')\n",
    "        \n",
    "        # Overlay lake polygon if provided\n",
    "        if lake_polygon is not None and polygon_crs is not None:\n",
    "            try:\n",
    "                # Transform polygon to image coordinates using the lake bounds transform\n",
    "                transformer = Transformer.from_crs(polygon_crs, src.crs, always_xy=True)\n",
    "                \n",
    "                # Transform polygon coordinates\n",
    "                transformed_coords = []\n",
    "                for x, y in lake_polygon.exterior.coords:\n",
    "                    # Transform geographic coordinates\n",
    "                    transformed_x, transformed_y = transformer.transform(x, y)\n",
    "                    \n",
    "                    # Convert to pixel coordinates relative to the cropped image\n",
    "                    pixel_x, pixel_y = ~out_transform * (transformed_x, transformed_y)\n",
    "                    \n",
    "                    # Downsample the coordinates\n",
    "                    transformed_coords.append((\n",
    "                        int(pixel_x / downsample_factor), \n",
    "                        int(pixel_y / downsample_factor)\n",
    "                    ))\n",
    "                \n",
    "                # Darker, less saturated red with lower opacity\n",
    "                red_outline_colors = [\n",
    "                    (180, 0, 0, 200),  # Outer line - darker, less opaque\n",
    "                    (180, 0, 0, 0),    # Middle line completely clear\n",
    "                    (180, 0, 0, 200)   # Inner line - darker, less opaque\n",
    "                ]\n",
    "                \n",
    "                # Draw thinner outlines\n",
    "                for thickness, color in zip(range(3, 0, -1), red_outline_colors):\n",
    "                    draw.polygon(transformed_coords, \n",
    "                                 outline=(*color[:3], color[3]), \n",
    "                                 fill=None)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error transforming polygon: {e}\")\n",
    "        \n",
    "        # Add title if provided\n",
    "        if title:\n",
    "            try:\n",
    "                font = ImageFont.truetype(\"arial.ttf\", 12)  # Reduced font size\n",
    "            except IOError:\n",
    "                font = ImageFont.load_default()\n",
    "            \n",
    "            # Add title with a semi-transparent background\n",
    "            text_bbox = draw.textbbox((0, 0), title, font=font)\n",
    "            text_width = text_bbox[2] - text_bbox[0]\n",
    "            text_height = text_bbox[3] - text_bbox[1]\n",
    "            \n",
    "            draw.rectangle([0, 0, text_width + 10, text_height + 10], \n",
    "                           fill=(200, 200, 200, 128))\n",
    "            \n",
    "            draw.text((5, 5), title, font=font, fill=(0, 0, 0))\n",
    "        \n",
    "        return np.array(pil_image)\n",
    "\n",
    "def get_lake_data_from_netcdf(netcdf_path, lake_id):\n",
    "    \"\"\"\n",
    "    Get lake data from vlen NetCDF structure.\n",
    "    \n",
    "    Args:\n",
    "        netcdf_path (str): Path to the NetCDF file\n",
    "        lake_id (int): ID of the lake to process\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (prefixes_list, None)  \n",
    "    \"\"\"\n",
    "    \n",
    "    with nc.Dataset(netcdf_path, 'r') as ncfile:\n",
    "        # Find the lake's index in the lake array\n",
    "        lake_ids = ncfile.variables['lake_id'][:]\n",
    "        try:\n",
    "            lake_idx = np.where(lake_ids == lake_id)[0][0]\n",
    "        except IndexError:\n",
    "            print(f\"Lake ID {lake_id} not found in the dataset.\")\n",
    "            return None, None\n",
    "        \n",
    "        # Get the count of observations for this lake\n",
    "        obs_count = ncfile.variables['obs_count'][lake_idx]\n",
    "        \n",
    "        if obs_count == 0:\n",
    "            print(f\"No observations found for Lake ID {lake_id}\")\n",
    "            return [], None\n",
    "        \n",
    "        # For variable-length arrays, we directly access the arrays for the specific lake\n",
    "        # Variable-length arrays are indexed by lake\n",
    "        datetimes = ncfile.variables['unix_time'][lake_idx]  # This returns a numpy array\n",
    "        \n",
    "        # Handle prefix access - it's a comma-separated string for each lake\n",
    "        prefix_raw = ncfile.variables['image_names_csv'][lake_idx]\n",
    "        \n",
    "        # Convert to string if needed\n",
    "        if isinstance(prefix_raw, bytes):\n",
    "            prefix_str = str(prefix_raw, 'utf-8')\n",
    "        else:\n",
    "            prefix_str = str(prefix_raw)\n",
    "        \n",
    "        # Split by commas to get individual prefixes\n",
    "        prefixes = [p.strip() for p in prefix_str.split(',')]\n",
    "        \n",
    "        # Return just the prefixes and None for observation_data to match expected return values\n",
    "        return prefixes, None\n",
    "\n",
    "def generate_lake_gif(netcdf_path, lake_id, base_dir, output_dir, shapefile_path, year=2021, \n",
    "                     downsample_factor=4, display_gif=False):\n",
    "    \"\"\"\n",
    "    Generate a GIF of lake images for a specific year with lake-centered zoom.\n",
    "    Updated to work with ragged array NetCDF structure.\n",
    "    \n",
    "    Args:\n",
    "        netcdf_path (str): Path to the ragged array NetCDF file\n",
    "        lake_id (int): ID of the lake to process\n",
    "        base_dir (str): Base directory for input images\n",
    "        output_dir (str): Directory to save output GIF\n",
    "        shapefile_path (str): Path to the lake shapefile\n",
    "        year (int, optional): Year to process. Defaults to 2021.\n",
    "        downsample_factor (int, optional): Factor to downsample images. Defaults to 4.\n",
    "        display_gif (bool, optional): Whether to display the GIF. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: Path to the generated GIF, or None if generation fails\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define the output path\n",
    "    output_path = os.path.join(output_dir, f'lake_{lake_id}_{year}_timelapse.gif')\n",
    "    \n",
    "    # Check if GIF already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"GIF already exists at {output_path}. Skipping generation.\")\n",
    "        \n",
    "        # Optional display\n",
    "        if display_gif:\n",
    "            try:\n",
    "                from IPython.display import Image, display\n",
    "                with open(output_path, 'rb') as f:\n",
    "                    display(Image(data=f.read(), format='gif'))\n",
    "            except ImportError:\n",
    "                print(\"Cannot display GIF: Not in a Jupyter environment\")\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    # Get lake polygon and its CRS\n",
    "    lake_polygon, polygon_crs = get_lake_polygon(shapefile_path, lake_id)\n",
    "    \n",
    "    if lake_polygon is None:\n",
    "        print(f\"Could not find polygon for Lake ID {lake_id}\")\n",
    "        return None\n",
    "    \n",
    "    # Get lake data using the vlen array approach\n",
    "    prefixes, observation_data = get_lake_data_from_netcdf(netcdf_path, lake_id)\n",
    "    \n",
    "    if prefixes is None:\n",
    "        print(f\"Lake ID {lake_id} not found in the dataset.\")\n",
    "        return None\n",
    "    \n",
    "    if not prefixes:\n",
    "        print(f\"No prefixes found for Lake ID {lake_id}\")\n",
    "        return None\n",
    "    \n",
    "    # Filter prefixes for the specified year\n",
    "    year_prefixes = [p for p in prefixes if str(year) in p]\n",
    "    \n",
    "    if not year_prefixes:\n",
    "        print(f\"No prefixes found for Lake ID {lake_id} in year {year}\")\n",
    "        return None\n",
    "    \n",
    "    # Find image paths and sort chronologically\n",
    "    image_paths = find_image_paths(base_dir, year_prefixes)\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(f\"No images found for Lake ID {lake_id} in year {year}\")\n",
    "        return None\n",
    "    \n",
    "    # Sort image paths chronologically\n",
    "    image_paths.sort()\n",
    "    \n",
    "    # Prepare images for GIF\n",
    "    images = []\n",
    "    \n",
    "    # Use the first image to get CRS for lake bounds\n",
    "    with rasterio.open(image_paths[0]) as first_src:\n",
    "        lake_bounds = create_lake_bounds(lake_polygon, polygon_crs, first_src.crs)\n",
    "    \n",
    "    for i, path in enumerate(tqdm(image_paths, desc=\"Processing images\")):\n",
    "        try:\n",
    "            # Extract filename as title\n",
    "            filename = os.path.splitext(os.path.basename(path))[0]\n",
    "            \n",
    "            # Add alphabetic label\n",
    "            label = chr(65 + i)  # A, B, C, etc.\n",
    "            title = f\"{label}: {filename}\"\n",
    "            \n",
    "            # Process image with lake bounds, polygon overlay, and title\n",
    "            processed_image = process_image(\n",
    "                path, lake_bounds, \n",
    "                lake_polygon=lake_polygon,\n",
    "                polygon_crs=polygon_crs,\n",
    "                downsample_factor=downsample_factor, \n",
    "                title=title\n",
    "            )\n",
    "            images.append(processed_image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {e}\")\n",
    "    \n",
    "    # Create GIF\n",
    "    if images:\n",
    "        imageio.mimsave(output_path, images, fps=2, loop=0)\n",
    "        print(f\"GIF saved to {output_path}\")\n",
    "        \n",
    "        # Optional display\n",
    "        if display_gif:\n",
    "            try:\n",
    "                from IPython.display import Image as IPythonImage, display\n",
    "                with open(output_path, 'rb') as f:\n",
    "                    display(IPythonImage(data=f.read(), format='gif'))\n",
    "            except ImportError:\n",
    "                print(\"Cannot display GIF: Not in a Jupyter environment\")\n",
    "        \n",
    "        return output_path\n",
    "    else:\n",
    "        print(\"No images could be processed.\")\n",
    "        return None\n",
    "\n",
    "# Example usage in __main__\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = r'D:\\planetscope_lake_ice\\Data (Unclassified)\\1 - Break Up Time Series Input'\n",
    "    output_dir = r'D:\\planetscope_lake_ice\\Data (Unclassified)\\2 - Break Up Time Series Output\\Visualization GIFs'\n",
    "    shapefile_path = r'D:\\planetscope_lake_ice\\Data (Validation)\\8 - Download ALPOD data here\\ALPODlakes.shp'\n",
    "    netcdf_path = r'D:\\planetscope_lake_ice\\Data (Unclassified)\\2 - Break Up Time Series Output\\lake_statistics_unlimited.nc'\n",
    "    \n",
    "    # Generate GIF for lake ID\n",
    "    generate_lake_gif(\n",
    "        netcdf_path=netcdf_path,\n",
    "        lake_id=lake_id,\n",
    "        base_dir=base_dir,\n",
    "        output_dir=output_dir,\n",
    "        shapefile_path=shapefile_path,\n",
    "        year=2023,\n",
    "        display_gif=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
