{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38538e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: compute 100×100 km bbox around (lat0, lon0)\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "def make_bbox(lat0, lon0):\n",
    "    \"\"\"\n",
    "    Returns a list of 5 (lon,lat) pairs defining a 100×100 km square \n",
    "    centered on (lat0, lon0), in GeoJSON Polygon order (closed ring).\n",
    "    \"\"\"\n",
    "    # local Azimuthal Equidistant projection centred on our point\n",
    "    aeqd = CRS.from_proj4(f\"+proj=aeqd +lat_0={lat0} +lon_0={lon0} +units=m +datum=WGS84\")\n",
    "    wgs84 = CRS.from_epsg(4326)\n",
    "    to_aeqd = Transformer.from_crs(wgs84, aeqd, always_xy=True)\n",
    "    to_wgs84 = Transformer.from_crs(aeqd, wgs84, always_xy=True)\n",
    "\n",
    "    half_side = 25_000  # metres (→ 100 km total side)\n",
    "    # corners in projected metres, starting lower-left and going counter-clockwise\n",
    "    proj_corners = [\n",
    "        (-half_side, -half_side),\n",
    "        (-half_side,  half_side),\n",
    "        ( half_side,  half_side),\n",
    "        ( half_side, -half_side),\n",
    "        (-half_side, -half_side),\n",
    "    ]\n",
    "    # transform back to lon/lat\n",
    "    ll_corners = []\n",
    "    for x, y in proj_corners:\n",
    "        lon, lat = to_wgs84.transform(x, y)\n",
    "        ll_corners.append((lon, lat))\n",
    "    return ll_corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: choose your centre point here:\n",
    "lat0, lon0 = 61.170966970204304, -161.90796944935005\n",
    "\n",
    "corners = make_bbox(lat0, lon0)\n",
    "\n",
    "# build your GeoJSON‐style dict\n",
    "rect = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [ corners ]\n",
    "}\n",
    "\n",
    "# print in the exact format you requested:\n",
    "print(\"rect = {\")\n",
    "print('    \"type\": \"Polygon\",')\n",
    "print('    \"coordinates\": [[')\n",
    "for lon, lat in corners:\n",
    "    print(f\"        [{lon:.6f}, {lat:.6f}],\")\n",
    "print(\"    ]]\")\n",
    "print(\"}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ab09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: render with Folium\n",
    "import folium\n",
    "\n",
    "# folium wants [(lat,lon),…]\n",
    "folium_points = [(lat, lon) for lon, lat in corners]\n",
    "\n",
    "m = folium.Map(location=(lat0, lon0), zoom_start=8)\n",
    "folium.Polygon(locations=folium_points, color=\"red\", weight=3, fill=False).add_to(m)\n",
    "m  # in Jupyter this will display the interactive map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18b5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import shutil\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "import backoff\n",
    "\n",
    "# —– load your key & session —–\n",
    "with open(r\"D:\\planetscope_lake_ice\\planet.yaml\", 'r') as f:\n",
    "    PLANET_API_KEY = yaml.safe_load(f)['api_key']\n",
    "\n",
    "BASE_URL = \"https://api.planet.com/data/v1\"\n",
    "session = requests.Session()\n",
    "session.auth = (PLANET_API_KEY, \"\")\n",
    "\n",
    "# Asset keys to download\n",
    "ASSET_KEYS = [\n",
    "    'ortho_analytic_4b_sr',\n",
    "    'ortho_analytic_4b_xml',\n",
    "    'ortho_udm2'\n",
    "]\n",
    "# Extension mapping\n",
    "EXT_MAP = {\n",
    "    'ortho_analytic_4b_sr': '.tif',\n",
    "    'ortho_analytic_4b_xml': '.xml',\n",
    "    'ortho_udm2': '.tif'\n",
    "}\n",
    "\n",
    "# Rate limiting parameters\n",
    "MAX_REQUESTS_PER_SECOND = 4  # Setting to 4 to stay safely below 5 req/sec limit\n",
    "REQUEST_INTERVAL = 1.0 / MAX_REQUESTS_PER_SECOND  # Time between requests\n",
    "last_request_time = time.time()\n",
    "\n",
    "# Custom exception for rate limit errors\n",
    "class RateLimitException(Exception):\n",
    "    pass\n",
    "\n",
    "# Rate limited session wrapper\n",
    "def rate_limited_request(method, url, **kwargs):\n",
    "    \"\"\"Make a rate-limited request and return the response.\"\"\"\n",
    "    global last_request_time\n",
    "    \n",
    "    # Calculate time to wait to respect rate limit\n",
    "    now = time.time()\n",
    "    elapsed = now - last_request_time\n",
    "    wait_time = max(0, REQUEST_INTERVAL - elapsed)\n",
    "    \n",
    "    if wait_time > 0:\n",
    "        time.sleep(wait_time)\n",
    "    \n",
    "    # Update last request time\n",
    "    last_request_time = time.time()\n",
    "    \n",
    "    # Make the request\n",
    "    response = method(url, **kwargs)\n",
    "    \n",
    "    # Check for rate limit errors\n",
    "    if response.status_code == 429:\n",
    "        retry_after = int(response.headers.get('Retry-After', 1))\n",
    "        raise RateLimitException(f\"Rate limit exceeded. Retry after {retry_after}s\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Backoff decorator for handling rate limits\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (RateLimitException, requests.exceptions.RequestException),\n",
    "    max_tries=10,  # Maximum number of retries\n",
    "    max_time=300,  # Maximum time to retry (5 minutes)\n",
    "    jitter=backoff.full_jitter,  # Add jitter to prevent thundering herd\n",
    "    on_backoff=lambda details: print(f\"Backing off {details['wait']:.1f}s after {details['tries']} tries. Error: {details['exception']}\")\n",
    ")\n",
    "def make_request_with_retry(method, url, **kwargs):\n",
    "    \"\"\"Make request with retry logic.\"\"\"\n",
    "    try:\n",
    "        return rate_limited_request(method, url, **kwargs)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # If it's a rate limit error, use specific handling\n",
    "        if e.response is not None and e.response.status_code == 429:\n",
    "            retry_after = int(e.response.headers.get('Retry-After', 5))\n",
    "            print(f\"Rate limit hit. Waiting {retry_after}s before retry...\")\n",
    "            time.sleep(retry_after)\n",
    "            raise RateLimitException(str(e))\n",
    "        raise\n",
    "\n",
    "def list_images(AOI, start, end, cloud_threshold):\n",
    "    \"\"\"\n",
    "    Search PSScene over AOI & date window, return list of features\n",
    "    filtered by cloud cover <= threshold. Handles pagination.\n",
    "    \"\"\"\n",
    "    print(f\"Searching for images from {start} to {end}...\")\n",
    "    features = []\n",
    "    body = {\n",
    "        \"item_types\": [\"PSScene\"],\n",
    "        \"filter\": {\n",
    "            \"type\": \"AndFilter\",\n",
    "            \"config\": [\n",
    "                {\"type\": \"GeometryFilter\", \"field_name\": \"geometry\", \"config\": AOI},\n",
    "                {\"type\": \"DateRangeFilter\", \"field_name\": \"acquired\",\n",
    "                 \"config\": {\"gte\": start, \"lte\": end}}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = make_request_with_retry(session.post, f\"{BASE_URL}/quick-search\", json=body)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        \n",
    "        page_count = 1\n",
    "        while True:\n",
    "            print(f\"Processing page {page_count} of search results...\")\n",
    "            page_count += 1\n",
    "            \n",
    "            for feat in data.get('features', []):\n",
    "                cloud_cover = feat['properties'].get('cloud_cover', 1.0)\n",
    "                if cloud_cover <= cloud_threshold:\n",
    "                    features.append(feat)\n",
    "            \n",
    "            next_link = data.get('_links', {}).get('next')\n",
    "            if not next_link:\n",
    "                print(\"No more pages to process.\")\n",
    "                break\n",
    "                \n",
    "            print(f\"Fetching next page from {next_link}\")\n",
    "            resp = make_request_with_retry(session.get, next_link)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image search: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"Total images found meeting criteria: {len(features)}\")\n",
    "    return features\n",
    "\n",
    "def batch_activate_assets(features, asset_keys, test_n_images=0):\n",
    "    \"\"\"\n",
    "    Activate all assets for all features in batch mode and return\n",
    "    a dict with activation status for each asset.\n",
    "    \"\"\"\n",
    "    if test_n_images > 0:\n",
    "        print(f\"TEST MODE: Limiting activation to first {test_n_images} images (out of {len(features)})\")\n",
    "        features = features[:test_n_images]\n",
    "    else:\n",
    "        print(f\"Activating assets for all {len(features)} images\")\n",
    "    \n",
    "    # Create a mapping of item_id -> asset_url and collect all assets that need activation\n",
    "    asset_mapping = {}\n",
    "    assets_to_activate = {}\n",
    "    \n",
    "    print(\"Checking asset statuses...\")\n",
    "    for feat in features:\n",
    "        item_id = feat['id']\n",
    "        item_type = feat['properties']['item_type']\n",
    "        assets_url = f\"{BASE_URL}/item-types/{item_type}/items/{item_id}/assets/\"\n",
    "        \n",
    "        # Store the assets URL for later use\n",
    "        asset_mapping[item_id] = assets_url\n",
    "        \n",
    "        # Check status of each asset\n",
    "        resp = make_request_with_retry(session.get, assets_url)\n",
    "        resp.raise_for_status()\n",
    "        assets_data = resp.json()\n",
    "        \n",
    "        for key in asset_keys:\n",
    "            if key in assets_data:\n",
    "                asset = assets_data[key]\n",
    "                if asset.get('status') != 'active':\n",
    "                    # If not active, add to activation list\n",
    "                    if key not in assets_to_activate:\n",
    "                        assets_to_activate[key] = []\n",
    "                    assets_to_activate[key].append((item_id, asset['_links']['activate']))\n",
    "    \n",
    "    # Activate all non-active assets serially to respect rate limits\n",
    "    for key, activation_list in assets_to_activate.items():\n",
    "        if not activation_list:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Activating {len(activation_list)} assets of type '{key}'...\")\n",
    "        \n",
    "        # Process activations with controlled concurrency\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:  # Limit concurrent requests\n",
    "            for batch_start in range(0, len(activation_list), 5):\n",
    "                batch = activation_list[batch_start:batch_start + 5]\n",
    "                print(f\"Processing activation batch {batch_start//5 + 1}/{(len(activation_list) + 4)//5}\")\n",
    "                \n",
    "                futures = []\n",
    "                for item_id, activate_url in batch:\n",
    "                    print(f\"Submitting activation request for {item_id} - {key}\")\n",
    "                    future = executor.submit(make_request_with_retry, session.post, activate_url)\n",
    "                    futures.append((item_id, key, future))\n",
    "                \n",
    "                # Wait for this batch to complete before starting next batch\n",
    "                for item_id, key, future in futures:\n",
    "                    try:\n",
    "                        resp = future.result()\n",
    "                        resp.raise_for_status()\n",
    "                        print(f\"Activation request sent for {item_id} - {key}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error activating {item_id} - {key}: {str(e)} - Will retry automatically\")\n",
    "    \n",
    "    print(\"All activation requests submitted, waiting for assets to become active...\")\n",
    "    \n",
    "    # Wait for all assets to become active\n",
    "    all_active = False\n",
    "    poll_count = 0\n",
    "    \n",
    "    while not all_active:\n",
    "        poll_count += 1\n",
    "        print(f\"\\nPolling attempt #{poll_count} - Checking activation status...\")\n",
    "        all_active = True\n",
    "        time.sleep(5)  # Wait 5 seconds between polls\n",
    "        \n",
    "        # Check status of each asset for each item\n",
    "        inactive_count = 0\n",
    "        for feat in features:\n",
    "            item_id = feat['id']\n",
    "            assets_url = asset_mapping[item_id]\n",
    "            \n",
    "            # Get current status of all assets\n",
    "            try:\n",
    "                resp = make_request_with_retry(session.get, assets_url)\n",
    "                resp.raise_for_status()\n",
    "                assets_data = resp.json()\n",
    "                \n",
    "                # Check if all required assets are active\n",
    "                for key in asset_keys:\n",
    "                    if key in assets_data:\n",
    "                        asset = assets_data[key]\n",
    "                        status = asset.get('status')\n",
    "                        \n",
    "                        if status != 'active':\n",
    "                            inactive_count += 1\n",
    "                            all_active = False\n",
    "                            if poll_count % 4 == 0:  # Only print detailed status every 20 seconds\n",
    "                                print(f\"Asset {key} for {item_id} is not yet active (status: {status})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking status for {item_id}: {str(e)} - Will continue polling\")\n",
    "                all_active = False  # Consider as not active if there was an error\n",
    "        \n",
    "        if inactive_count > 0:\n",
    "            print(f\"Still waiting for {inactive_count} assets to become active...\")\n",
    "        else:\n",
    "            print(\"All assets are now active!\")\n",
    "    \n",
    "    return asset_mapping\n",
    "\n",
    "def download_assets_batch(features, asset_mapping, base_folder, order_name, test_n_images=0):\n",
    "    \"\"\"\n",
    "    Download all specified assets for given features into a single zip file named {order_name}.zip.\n",
    "    Assumes all assets have been activated already.\n",
    "    \"\"\"\n",
    "    print(f\"\\nPreparing to download assets for order: {order_name}\")\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    zip_path = os.path.join(base_folder, f\"{order_name}.zip\")\n",
    "    print(f\"Assets will be saved to: {zip_path}\")\n",
    "    \n",
    "    # Apply test limit if specified\n",
    "    if test_n_images > 0:\n",
    "        print(f\"TEST MODE: Limiting download to first {test_n_images} images (out of {len(features)})\")\n",
    "        features = features[:test_n_images]\n",
    "\n",
    "    count = 0\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'w') as z:\n",
    "            for i, feat in enumerate(features, 1):\n",
    "                item_id = feat['id']\n",
    "                assets_url = asset_mapping[item_id]\n",
    "                print(f\"\\nDownloading assets for image {i}/{len(features)}: {item_id}\")\n",
    "                \n",
    "                # Get all assets for this item\n",
    "                resp = make_request_with_retry(session.get, assets_url)\n",
    "                resp.raise_for_status()\n",
    "                assets_data = resp.json()\n",
    "                \n",
    "                for key in ASSET_KEYS:\n",
    "                    if key not in assets_data:\n",
    "                        print(f\"Asset {key} not available for {item_id}\")\n",
    "                        continue\n",
    "                        \n",
    "                    asset = assets_data[key]\n",
    "                    print(f\"Downloading asset {key} for {item_id}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        download_start = time.time()\n",
    "                        resp = make_request_with_retry(session.get, asset['location'])\n",
    "                        resp.raise_for_status()\n",
    "                        data = resp.content\n",
    "                        download_time = time.time() - download_start\n",
    "                        data_size_mb = len(data)/1024/1024\n",
    "                        print(f\"Download complete: {data_size_mb:.2f} MB in {download_time:.2f}s ({data_size_mb/download_time:.2f} MB/s)\")\n",
    "\n",
    "                        ext = EXT_MAP.get(key, '')\n",
    "                        filename = f\"{item_id}_{key}{ext}\"\n",
    "                        print(f\"Adding {filename} to zip archive...\")\n",
    "                        z.writestr(filename, data)\n",
    "                        count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error downloading asset {key} for {item_id}: {str(e)} - Will retry\")\n",
    "                        # Retry this specific asset download\n",
    "                        retry_attempts = 3\n",
    "                        for attempt in range(retry_attempts):\n",
    "                            try:\n",
    "                                print(f\"Retry attempt {attempt+1}/{retry_attempts} for {item_id} - {key}\")\n",
    "                                time.sleep((attempt + 1) * 2)  # Increasing backoff\n",
    "                                resp = make_request_with_retry(session.get, asset['location'])\n",
    "                                resp.raise_for_status()\n",
    "                                data = resp.content\n",
    "                                ext = EXT_MAP.get(key, '')\n",
    "                                filename = f\"{item_id}_{key}{ext}\"\n",
    "                                z.writestr(filename, data)\n",
    "                                count += 1\n",
    "                                print(f\"Retry successful for {item_id} - {key}\")\n",
    "                                break\n",
    "                            except Exception as retry_e:\n",
    "                                print(f\"Retry {attempt+1} failed: {str(retry_e)}\")\n",
    "                                if attempt == retry_attempts - 1:\n",
    "                                    print(f\"All retries failed for {item_id} - {key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch download: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"\\nDownload complete for order {order_name}. Total assets downloaded: {count}\")\n",
    "    return count\n",
    "\n",
    "def download_seasonal_orders(AOI, study_site_folder, backup_folder,\n",
    "                             years, seasons, cloud_threshold=0.5, test_n_images=0):\n",
    "    \"\"\"\n",
    "    For each year and season, list images, activate assets in batch, download assets, \n",
    "    log to a single download_log.txt, and build a master image_roster.csv.\n",
    "    \n",
    "    Parameters:\n",
    "    - test_n_images: If > 0, only process the first n images for each season/year\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Starting download process at {datetime.now().isoformat()}\")\n",
    "    print(f\"Study site folder: {study_site_folder}\")\n",
    "    print(f\"Backup folder: {backup_folder}\")\n",
    "    print(f\"Cloud threshold: {cloud_threshold}\")\n",
    "    print(f\"Rate limit: {MAX_REQUESTS_PER_SECOND} requests per second\")\n",
    "    if test_n_images > 0:\n",
    "        print(f\"TEST MODE ACTIVE: Limited to first {test_n_images} images per season/year\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "    \n",
    "    # Single log file in base\n",
    "    log_path = os.path.join(study_site_folder, 'download_log.txt')\n",
    "    # For CSV: collect { order_name: [image_id, ...] }\n",
    "    roster_dict = {}\n",
    "\n",
    "    for year in years:\n",
    "        for season, (start_tmpl, end_tmpl) in seasons.items():\n",
    "            start = start_tmpl.format(year=year)\n",
    "            end = end_tmpl.format(year=year)\n",
    "            order_name = f\"{season}_{year}\"\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"Processing {order_name}: {start} to {end}\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "\n",
    "            # 1) list & filter\n",
    "            feats = list_images(AOI, start, end, cloud_threshold)\n",
    "            image_ids = [f['id'] for f in feats]\n",
    "            roster_dict[order_name] = image_ids\n",
    "            print(f\"Found {len(image_ids)} images meeting criteria.\")\n",
    "            \n",
    "            if not feats:\n",
    "                print(f\"No images found for {order_name}, skipping to next season/year.\")\n",
    "                continue\n",
    "\n",
    "            # 2) choose storage\n",
    "            free = shutil.disk_usage(study_site_folder).free\n",
    "            if free < 5 * 1024**3:\n",
    "                base_folder = backup_folder\n",
    "                print(f\"WARNING: Less than 5 GB free space in primary folder\")\n",
    "                print(f\"Switching to backup folder: {backup_folder}\")\n",
    "            else:\n",
    "                base_folder = study_site_folder\n",
    "                print(f\"Using primary folder: {study_site_folder}\")\n",
    "                print(f\"Free space: {free / 1024**3:.2f} GB\")\n",
    "\n",
    "            # 3) activate assets in batch for the entire season\n",
    "            print(f\"\\nActivating assets for {order_name}...\")\n",
    "            t0_activation = time.time()\n",
    "            try:\n",
    "                asset_mapping = batch_activate_assets(feats, ASSET_KEYS, test_n_images)\n",
    "                activation_time = time.time() - t0_activation\n",
    "                print(f\"All assets activated for {order_name} in {activation_time:.1f} seconds\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during activation for {order_name}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # 4) download activated assets\n",
    "            t0_download = time.time()\n",
    "            try:\n",
    "                num_assets = download_assets_batch(feats, asset_mapping, base_folder, order_name, test_n_images)\n",
    "                download_time = time.time() - t0_download\n",
    "                total_time = time.time() - t0_activation\n",
    "                print(f\"Download complete for {order_name}\")\n",
    "                print(f\"Downloaded {num_assets} assets in {download_time:.1f} seconds\")\n",
    "                print(f\"Total processing time: {total_time:.1f} seconds\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during download for {order_name}: {str(e)}\")\n",
    "                total_time = time.time() - t0_activation\n",
    "                num_assets = 0\n",
    "\n",
    "            # 5) append to log\n",
    "            print(f\"Updating download log at {log_path}\")\n",
    "            with open(log_path, 'a') as logf:\n",
    "                log_entry = (\n",
    "                    f\"{datetime.now().isoformat()} | {order_name} | \"\n",
    "                    f\"images={len(image_ids)} | assets={num_assets} | \"\n",
    "                    f\"time_s={total_time:.1f}\"\n",
    "                )\n",
    "                if test_n_images > 0:\n",
    "                    log_entry += f\" | test_n_images_override={test_n_images}\"\n",
    "                log_entry += \"\\n\"\n",
    "                logf.write(log_entry)\n",
    "\n",
    "    # 6) write master CSV\n",
    "    csv_path = os.path.join(study_site_folder, 'image_roster.csv')\n",
    "    print(f\"\\nGenerating master image roster at {csv_path}\")\n",
    "    all_orders = list(roster_dict.keys())\n",
    "    max_len = max(len(v) for v in roster_dict.values()) if roster_dict else 0\n",
    "\n",
    "    with open(csv_path, 'w', newline='') as csvf:\n",
    "        writer = csv.writer(csvf)\n",
    "        writer.writerow(all_orders)\n",
    "        for i in range(max_len):\n",
    "            row = [roster_dict[ord][i] if i < len(roster_dict[ord]) else ''\n",
    "                   for ord in all_orders]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Processing complete at {datetime.now().isoformat()}\")\n",
    "    print(f\"Log updated at: {log_path}\")\n",
    "    print(f\"Master roster CSV at: {csv_path}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "def extract_all_zips(folder_path):\n",
    "    \"\"\"\n",
    "    Extract all zip files found in the specified folder,\n",
    "    then delete the original zip files after successful extraction.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Starting zip extraction process for {folder_path} at {datetime.now().isoformat()}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder {folder_path} does not exist, skipping.\")\n",
    "        return\n",
    "    \n",
    "    # Find all zip files in the folder\n",
    "    zip_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) \n",
    "                if f.endswith('.zip') and os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if not zip_files:\n",
    "        print(f\"No zip files found in {folder_path}.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(zip_files)} zip files to extract.\")\n",
    "    \n",
    "    # Process each zip file\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for zip_path in zip_files:\n",
    "        try:\n",
    "            zip_name = os.path.basename(zip_path)\n",
    "            extract_folder = os.path.join(folder_path, os.path.splitext(zip_name)[0])\n",
    "            \n",
    "            print(f\"\\nProcessing: {zip_name}\")\n",
    "            print(f\"Extracting to: {extract_folder}\")\n",
    "            \n",
    "            # Create extraction folder\n",
    "            os.makedirs(extract_folder, exist_ok=True)\n",
    "            \n",
    "            # Extract the zip file\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                file_count = len(zip_ref.namelist())\n",
    "                print(f\"Zip contains {file_count} files\")\n",
    "                zip_ref.extractall(extract_folder)\n",
    "            \n",
    "            # Verify extraction\n",
    "            extracted_files = []\n",
    "            for root, _, files in os.walk(extract_folder):\n",
    "                extracted_files.extend(files)\n",
    "            \n",
    "            if len(extracted_files) == file_count:\n",
    "                print(f\"Extraction successful: {file_count} files extracted\")\n",
    "                print(f\"Deleting original zip file: {zip_path}\")\n",
    "                os.remove(zip_path)\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"WARNING: Extraction verification failed. Expected {file_count} files but found {len(extracted_files)}\")\n",
    "                print(f\"Original zip file {zip_path} preserved for manual inspection\")\n",
    "                fail_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR extracting {zip_path}: {str(e)}\")\n",
    "            print(f\"Original zip file preserved for manual inspection\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Extraction process completed for {folder_path} at {datetime.now().isoformat()}\")\n",
    "    print(f\"Successfully processed: {success_count} zip files\")\n",
    "    if fail_count > 0:\n",
    "        print(f\"Failed: {fail_count} zip files - see above for details\")\n",
    "    print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1dd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting download process at 2025-04-28T12:56:38.925189\n",
      "Study site folder: D:\\planetscope_lake_ice\\Data\n",
      "Backup folder: C:\\Users\\nj142\\Desktop\\Fallback\n",
      "Cloud threshold: 0.5\n",
      "Rate limit: 4 requests per second\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Breakup_2019: 2019-04-15T00:00:00Z to 2019-06-15T23:59:59Z\n",
      "============================================================\n",
      "Searching for images from 2019-04-15T00:00:00Z to 2019-06-15T23:59:59Z...\n",
      "Processing page 1 of search results...\n",
      "No more pages to process.\n",
      "Total images found meeting criteria: 172\n",
      "Found 172 images meeting criteria.\n",
      "Using primary folder: D:\\planetscope_lake_ice\\Data\n",
      "Free space: 1483.57 GB\n",
      "\n",
      "Activating assets for Breakup_2019...\n",
      "Activating assets for all 172 images\n",
      "Checking asset statuses...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m primary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mplanetscope_lake_ice\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m backup \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnj142\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFallback\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 21\u001b[0m download_seasonal_orders(\n\u001b[0;32m     22\u001b[0m     AOI\u001b[38;5;241m=\u001b[39mrect,\n\u001b[0;32m     23\u001b[0m     study_site_folder\u001b[38;5;241m=\u001b[39mprimary,\n\u001b[0;32m     24\u001b[0m     backup_folder\u001b[38;5;241m=\u001b[39mbackup,\n\u001b[0;32m     25\u001b[0m     years\u001b[38;5;241m=\u001b[39myears,\n\u001b[0;32m     26\u001b[0m     seasons\u001b[38;5;241m=\u001b[39mseasons,\n\u001b[0;32m     27\u001b[0m     cloud_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m extract_all_zips(primary)\n\u001b[0;32m     31\u001b[0m extract_all_zips(backup)\n",
      "Cell \u001b[1;32mIn[6], line 389\u001b[0m, in \u001b[0;36mdownload_seasonal_orders\u001b[1;34m(AOI, study_site_folder, backup_folder, years, seasons, cloud_threshold, test_n_images)\u001b[0m\n\u001b[0;32m    387\u001b[0m t0_activation \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m     asset_mapping \u001b[38;5;241m=\u001b[39m batch_activate_assets(feats, ASSET_KEYS, test_n_images)\n\u001b[0;32m    390\u001b[0m     activation_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0_activation\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll assets activated for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 166\u001b[0m, in \u001b[0;36mbatch_activate_assets\u001b[1;34m(features, asset_keys, test_n_images)\u001b[0m\n\u001b[0;32m    163\u001b[0m asset_mapping[item_id] \u001b[38;5;241m=\u001b[39m assets_url\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Check status of each asset\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m resp \u001b[38;5;241m=\u001b[39m make_request_with_retry(session\u001b[38;5;241m.\u001b[39mget, assets_url)\n\u001b[0;32m    167\u001b[0m resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    168\u001b[0m assets_data \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\site-packages\\backoff\\_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[0;32m    102\u001b[0m }\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "Cell \u001b[1;32mIn[6], line 81\u001b[0m, in \u001b[0;36mmake_request_with_retry\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make request with retry logic.\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rate_limited_request(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# If it's a rate limit error, use specific handling\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n",
      "Cell \u001b[1;32mIn[6], line 60\u001b[0m, in \u001b[0;36mrate_limited_request\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m last_request_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Make the request\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m response \u001b[38;5;241m=\u001b[39m method(url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Check for rate limit errors\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    586\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    590\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    591\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    592\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    593\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    594\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    595\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    596\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    599\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    600\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    601\u001b[0m     )\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\nj142\\AppData\\Local\\anaconda3\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rect = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [[\n",
    "        [-162.369230, 60.945820],\n",
    "        [-162.375829, 61.394512],\n",
    "        [-161.440110, 61.394512],\n",
    "        [-161.446709, 60.945820],\n",
    "        [-162.369230, 60.945820],\n",
    "    ]]\n",
    "}\n",
    "\n",
    "years = [2020, 2021, 2022, 2023, 2024]\n",
    "seasons = {\n",
    "    \"Breakup\": (\"{year}-04-15T00:00:00Z\", \"{year}-06-15T23:59:59Z\"),\n",
    "    \"Freezeup\": (\"{year}-10-01T00:00:00Z\", \"{year}-11-30T23:59:59Z\")\n",
    "}\n",
    "\n",
    "primary = r\"D:\\planetscope_lake_ice\\Data\\Input\\YKD\"\n",
    "backup = r\"C:\\Users\\nj142\\Desktop\\Fallback\"\n",
    "\n",
    "download_seasonal_orders(\n",
    "    AOI=rect,\n",
    "    study_site_folder=primary,\n",
    "    backup_folder=backup,\n",
    "    years=years,\n",
    "    seasons=seasons,\n",
    "    cloud_threshold=0.5,\n",
    ")\n",
    "\n",
    "extract_all_zips(primary)\n",
    "extract_all_zips(backup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d9b8a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting zip extraction process for D:\\planetscope_lake_ice\\Data at 2025-04-28T12:31:13.834562\n",
      "============================================================\n",
      "\n",
      "Found 1 zip files to extract.\n",
      "\n",
      "Processing: Freezeup_2019.zip\n",
      "Extracting to: D:\\planetscope_lake_ice\\Data\\Freezeup_2019\n",
      "Zip contains 367 files\n",
      "Extraction successful: 367 files extracted\n",
      "Deleting original zip file: D:\\planetscope_lake_ice\\Data\\Freezeup_2019.zip\n",
      "\n",
      "============================================================\n",
      "Extraction process completed for D:\\planetscope_lake_ice\\Data at 2025-04-28T12:54:38.366747\n",
      "Successfully processed: 1 zip files\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "extract_all_zips(r\"D:\\planetscope_lake_ice\\Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
