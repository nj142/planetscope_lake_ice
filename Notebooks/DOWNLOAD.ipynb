{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38538e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: compute 100×100 km bbox around (lat0, lon0)\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "def make_bbox(lat0, lon0):\n",
    "    \"\"\"\n",
    "    Returns a list of 5 (lon,lat) pairs defining a 100×100 km square \n",
    "    centered on (lat0, lon0), in GeoJSON Polygon order (closed ring).\n",
    "    \"\"\"\n",
    "    # local Azimuthal Equidistant projection centred on our point\n",
    "    aeqd = CRS.from_proj4(f\"+proj=aeqd +lat_0={lat0} +lon_0={lon0} +units=m +datum=WGS84\")\n",
    "    wgs84 = CRS.from_epsg(4326)\n",
    "    to_aeqd = Transformer.from_crs(wgs84, aeqd, always_xy=True)\n",
    "    to_wgs84 = Transformer.from_crs(aeqd, wgs84, always_xy=True)\n",
    "\n",
    "    half_side = 25_000  # metres (→ 100 km total side)\n",
    "    # corners in projected metres, starting lower-left and going counter-clockwise\n",
    "    proj_corners = [\n",
    "        (-half_side, -half_side),\n",
    "        (-half_side,  half_side),\n",
    "        ( half_side,  half_side),\n",
    "        ( half_side, -half_side),\n",
    "        (-half_side, -half_side),\n",
    "    ]\n",
    "    # transform back to lon/lat\n",
    "    ll_corners = []\n",
    "    for x, y in proj_corners:\n",
    "        lon, lat = to_wgs84.transform(x, y)\n",
    "        ll_corners.append((lon, lat))\n",
    "    return ll_corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: choose your centre point here:\n",
    "lat0, lon0 = 61.170966970204304, -161.90796944935005\n",
    "\n",
    "corners = make_bbox(lat0, lon0)\n",
    "\n",
    "# build your GeoJSON‐style dict\n",
    "rect = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [ corners ]\n",
    "}\n",
    "\n",
    "# print in the exact format you requested:\n",
    "print(\"rect = {\")\n",
    "print('    \"type\": \"Polygon\",')\n",
    "print('    \"coordinates\": [[')\n",
    "for lon, lat in corners:\n",
    "    print(f\"        [{lon:.6f}, {lat:.6f}],\")\n",
    "print(\"    ]]\")\n",
    "print(\"}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ab09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: render with Folium\n",
    "import folium\n",
    "\n",
    "# folium wants [(lat,lon),…]\n",
    "folium_points = [(lat, lon) for lon, lat in corners]\n",
    "\n",
    "m = folium.Map(location=(lat0, lon0), zoom_start=8)\n",
    "folium.Polygon(locations=folium_points, color=\"red\", weight=3, fill=False).add_to(m)\n",
    "m  # in Jupyter this will display the interactive map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b5834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import shutil\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "import backoff\n",
    "\n",
    "# —– load your key & session —–\n",
    "with open(r\"D:\\planetscope_lake_ice\\planet.yaml\", 'r') as f:\n",
    "    PLANET_API_KEY = yaml.safe_load(f)['api_key']\n",
    "\n",
    "BASE_URL = \"https://api.planet.com/data/v1\"\n",
    "session = requests.Session()\n",
    "session.auth = (PLANET_API_KEY, \"\")\n",
    "\n",
    "# Asset keys to download\n",
    "ASSET_KEYS = [\n",
    "    'ortho_analytic_4b_sr',\n",
    "    'ortho_analytic_4b_xml',\n",
    "    'ortho_udm2'\n",
    "]\n",
    "# Extension mapping\n",
    "EXT_MAP = {\n",
    "    'ortho_analytic_4b_sr': '.tif',\n",
    "    'ortho_analytic_4b_xml': '.xml',\n",
    "    'ortho_udm2': '.tif'\n",
    "}\n",
    "\n",
    "# Rate limiting parameters\n",
    "MAX_REQUESTS_PER_SECOND = 4  # Setting to 4 to stay safely below 5 req/sec limit\n",
    "REQUEST_INTERVAL = 1.0 / MAX_REQUESTS_PER_SECOND  # Time between requests\n",
    "last_request_time = time.time()\n",
    "\n",
    "# Custom exception for rate limit errors\n",
    "class RateLimitException(Exception):\n",
    "    pass\n",
    "\n",
    "# Rate limited session wrapper\n",
    "def rate_limited_request(method, url, **kwargs):\n",
    "    \"\"\"Make a rate-limited request and return the response.\"\"\"\n",
    "    global last_request_time\n",
    "    \n",
    "    # Calculate time to wait to respect rate limit\n",
    "    now = time.time()\n",
    "    elapsed = now - last_request_time\n",
    "    wait_time = max(0, REQUEST_INTERVAL - elapsed)\n",
    "    \n",
    "    if wait_time > 0:\n",
    "        time.sleep(wait_time)\n",
    "    \n",
    "    # Update last request time\n",
    "    last_request_time = time.time()\n",
    "    \n",
    "    # Make the request\n",
    "    response = method(url, **kwargs)\n",
    "    \n",
    "    # Check for rate limit errors\n",
    "    if response.status_code == 429:\n",
    "        retry_after = int(response.headers.get('Retry-After', 1))\n",
    "        raise RateLimitException(f\"Rate limit exceeded. Retry after {retry_after}s\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Backoff decorator for handling rate limits\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (RateLimitException, requests.exceptions.RequestException),\n",
    "    max_tries=10,  # Maximum number of retries\n",
    "    max_time=300,  # Maximum time to retry (5 minutes)\n",
    "    jitter=backoff.full_jitter,  # Add jitter to prevent thundering herd\n",
    "    on_backoff=lambda details: print(f\"Backing off {details['wait']:.1f}s after {details['tries']} tries. Error: {details['exception']}\")\n",
    ")\n",
    "def make_request_with_retry(method, url, **kwargs):\n",
    "    \"\"\"Make request with retry logic.\"\"\"\n",
    "    try:\n",
    "        return rate_limited_request(method, url, **kwargs)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # If it's a rate limit error, use specific handling\n",
    "        if e.response is not None and e.response.status_code == 429:\n",
    "            retry_after = int(e.response.headers.get('Retry-After', 5))\n",
    "            print(f\"Rate limit hit. Waiting {retry_after}s before retry...\")\n",
    "            time.sleep(retry_after)\n",
    "            raise RateLimitException(str(e))\n",
    "        raise\n",
    "\n",
    "def list_images(AOI, start, end, cloud_threshold):\n",
    "    \"\"\"\n",
    "    Search PSScene over AOI & date window, return list of features\n",
    "    filtered by cloud cover <= threshold. Handles pagination.\n",
    "    \"\"\"\n",
    "    print(f\"Searching for images from {start} to {end}...\")\n",
    "    features = []\n",
    "    body = {\n",
    "        \"item_types\": [\"PSScene\"],\n",
    "        \"filter\": {\n",
    "            \"type\": \"AndFilter\",\n",
    "            \"config\": [\n",
    "                {\"type\": \"GeometryFilter\", \"field_name\": \"geometry\", \"config\": AOI},\n",
    "                {\"type\": \"DateRangeFilter\", \"field_name\": \"acquired\",\n",
    "                 \"config\": {\"gte\": start, \"lte\": end}}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        resp = make_request_with_retry(session.post, f\"{BASE_URL}/quick-search\", json=body)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        \n",
    "        page_count = 1\n",
    "        while True:\n",
    "            print(f\"Processing page {page_count} of search results...\")\n",
    "            page_count += 1\n",
    "            \n",
    "            for feat in data.get('features', []):\n",
    "                cloud_cover = feat['properties'].get('cloud_cover', 1.0)\n",
    "                if cloud_cover <= cloud_threshold:\n",
    "                    features.append(feat)\n",
    "            \n",
    "            next_link = data.get('_links', {}).get('next')\n",
    "            if not next_link:\n",
    "                print(\"No more pages to process.\")\n",
    "                break\n",
    "                \n",
    "            print(f\"Fetching next page from {next_link}\")\n",
    "            resp = make_request_with_retry(session.get, next_link)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image search: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"Total images found meeting criteria: {len(features)}\")\n",
    "    return features\n",
    "\n",
    "def batch_activate_assets(features, asset_keys, test_n_images=0):\n",
    "    \"\"\"\n",
    "    Activate all assets for all features in batch mode and return\n",
    "    a dict with activation status for each asset.\n",
    "    \"\"\"\n",
    "    if test_n_images > 0:\n",
    "        print(f\"TEST MODE: Limiting activation to first {test_n_images} images (out of {len(features)})\")\n",
    "        features = features[:test_n_images]\n",
    "    else:\n",
    "        print(f\"Activating assets for all {len(features)} images\")\n",
    "    \n",
    "    # Create a mapping of item_id -> asset_url and collect all assets that need activation\n",
    "    asset_mapping = {}\n",
    "    assets_to_activate = {}\n",
    "    \n",
    "    print(\"Checking asset statuses...\")\n",
    "    for feat in features:\n",
    "        item_id = feat['id']\n",
    "        item_type = feat['properties']['item_type']\n",
    "        assets_url = f\"{BASE_URL}/item-types/{item_type}/items/{item_id}/assets/\"\n",
    "        \n",
    "        # Store the assets URL for later use\n",
    "        asset_mapping[item_id] = assets_url\n",
    "        \n",
    "        # Check status of each asset\n",
    "        resp = make_request_with_retry(session.get, assets_url)\n",
    "        resp.raise_for_status()\n",
    "        assets_data = resp.json()\n",
    "        \n",
    "        for key in asset_keys:\n",
    "            if key in assets_data:\n",
    "                asset = assets_data[key]\n",
    "                if asset.get('status') != 'active':\n",
    "                    # If not active, add to activation list\n",
    "                    if key not in assets_to_activate:\n",
    "                        assets_to_activate[key] = []\n",
    "                    assets_to_activate[key].append((item_id, asset['_links']['activate']))\n",
    "    \n",
    "    # Activate all non-active assets serially to respect rate limits\n",
    "    for key, activation_list in assets_to_activate.items():\n",
    "        if not activation_list:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Activating {len(activation_list)} assets of type '{key}'...\")\n",
    "        \n",
    "        # Process activations with controlled concurrency\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:  # Limit concurrent requests\n",
    "            for batch_start in range(0, len(activation_list), 5):\n",
    "                batch = activation_list[batch_start:batch_start + 5]\n",
    "                print(f\"Processing activation batch {batch_start//5 + 1}/{(len(activation_list) + 4)//5}\")\n",
    "                \n",
    "                futures = []\n",
    "                for item_id, activate_url in batch:\n",
    "                    print(f\"Submitting activation request for {item_id} - {key}\")\n",
    "                    future = executor.submit(make_request_with_retry, session.post, activate_url)\n",
    "                    futures.append((item_id, key, future))\n",
    "                \n",
    "                # Wait for this batch to complete before starting next batch\n",
    "                for item_id, key, future in futures:\n",
    "                    try:\n",
    "                        resp = future.result()\n",
    "                        resp.raise_for_status()\n",
    "                        print(f\"Activation request sent for {item_id} - {key}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error activating {item_id} - {key}: {str(e)} - Will retry automatically\")\n",
    "    \n",
    "    print(\"All activation requests submitted, waiting for assets to become active...\")\n",
    "    \n",
    "    # Wait for all assets to become active\n",
    "    all_active = False\n",
    "    poll_count = 0\n",
    "    \n",
    "    while not all_active:\n",
    "        poll_count += 1\n",
    "        print(f\"\\nPolling attempt #{poll_count} - Checking activation status...\")\n",
    "        all_active = True\n",
    "        time.sleep(5)  # Wait 5 seconds between polls\n",
    "        \n",
    "        # Check status of each asset for each item\n",
    "        inactive_count = 0\n",
    "        for feat in features:\n",
    "            item_id = feat['id']\n",
    "            assets_url = asset_mapping[item_id]\n",
    "            \n",
    "            # Get current status of all assets\n",
    "            try:\n",
    "                resp = make_request_with_retry(session.get, assets_url)\n",
    "                resp.raise_for_status()\n",
    "                assets_data = resp.json()\n",
    "                \n",
    "                # Check if all required assets are active\n",
    "                for key in asset_keys:\n",
    "                    if key in assets_data:\n",
    "                        asset = assets_data[key]\n",
    "                        status = asset.get('status')\n",
    "                        \n",
    "                        if status != 'active':\n",
    "                            inactive_count += 1\n",
    "                            all_active = False\n",
    "                            if poll_count % 4 == 0:  # Only print detailed status every 20 seconds\n",
    "                                print(f\"Asset {key} for {item_id} is not yet active (status: {status})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking status for {item_id}: {str(e)} - Will continue polling\")\n",
    "                all_active = False  # Consider as not active if there was an error\n",
    "        \n",
    "        if inactive_count > 0:\n",
    "            print(f\"Still waiting for {inactive_count} assets to become active...\")\n",
    "        else:\n",
    "            print(\"All assets are now active!\")\n",
    "    \n",
    "    return asset_mapping\n",
    "\n",
    "def download_assets_batch(features, asset_mapping, base_folder, order_name, test_n_images=0):\n",
    "    \"\"\"\n",
    "    Download all specified assets for given features into a single zip file named {order_name}.zip.\n",
    "    Assumes all assets have been activated already.\n",
    "    \"\"\"\n",
    "    print(f\"\\nPreparing to download assets for order: {order_name}\")\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    zip_path = os.path.join(base_folder, f\"{order_name}.zip\")\n",
    "    print(f\"Assets will be saved to: {zip_path}\")\n",
    "    \n",
    "    # Apply test limit if specified\n",
    "    if test_n_images > 0:\n",
    "        print(f\"TEST MODE: Limiting download to first {test_n_images} images (out of {len(features)})\")\n",
    "        features = features[:test_n_images]\n",
    "\n",
    "    count = 0\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'w') as z:\n",
    "            for i, feat in enumerate(features, 1):\n",
    "                item_id = feat['id']\n",
    "                assets_url = asset_mapping[item_id]\n",
    "                print(f\"\\nDownloading assets for image {i}/{len(features)}: {item_id}\")\n",
    "                \n",
    "                # Get all assets for this item\n",
    "                resp = make_request_with_retry(session.get, assets_url)\n",
    "                resp.raise_for_status()\n",
    "                assets_data = resp.json()\n",
    "                \n",
    "                for key in ASSET_KEYS:\n",
    "                    if key not in assets_data:\n",
    "                        print(f\"Asset {key} not available for {item_id}\")\n",
    "                        continue\n",
    "                        \n",
    "                    asset = assets_data[key]\n",
    "                    print(f\"Downloading asset {key} for {item_id}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        download_start = time.time()\n",
    "                        resp = make_request_with_retry(session.get, asset['location'])\n",
    "                        resp.raise_for_status()\n",
    "                        data = resp.content\n",
    "                        download_time = time.time() - download_start\n",
    "                        data_size_mb = len(data)/1024/1024\n",
    "                        print(f\"Download complete: {data_size_mb:.2f} MB in {download_time:.2f}s ({data_size_mb/download_time:.2f} MB/s)\")\n",
    "\n",
    "                        ext = EXT_MAP.get(key, '')\n",
    "                        filename = f\"{item_id}_{key}{ext}\"\n",
    "                        print(f\"Adding {filename} to zip archive...\")\n",
    "                        z.writestr(filename, data)\n",
    "                        count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error downloading asset {key} for {item_id}: {str(e)} - Will retry\")\n",
    "                        # Retry this specific asset download\n",
    "                        retry_attempts = 3\n",
    "                        for attempt in range(retry_attempts):\n",
    "                            try:\n",
    "                                print(f\"Retry attempt {attempt+1}/{retry_attempts} for {item_id} - {key}\")\n",
    "                                time.sleep((attempt + 1) * 2)  # Increasing backoff\n",
    "                                resp = make_request_with_retry(session.get, asset['location'])\n",
    "                                resp.raise_for_status()\n",
    "                                data = resp.content\n",
    "                                ext = EXT_MAP.get(key, '')\n",
    "                                filename = f\"{item_id}_{key}{ext}\"\n",
    "                                z.writestr(filename, data)\n",
    "                                count += 1\n",
    "                                print(f\"Retry successful for {item_id} - {key}\")\n",
    "                                break\n",
    "                            except Exception as retry_e:\n",
    "                                print(f\"Retry {attempt+1} failed: {str(retry_e)}\")\n",
    "                                if attempt == retry_attempts - 1:\n",
    "                                    print(f\"All retries failed for {item_id} - {key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch download: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"\\nDownload complete for order {order_name}. Total assets downloaded: {count}\")\n",
    "    return count\n",
    "\n",
    "def download_seasonal_orders(AOI, study_site_folder, backup_folder,\n",
    "                             years, seasons, cloud_threshold=0.5, test_n_images=0):\n",
    "    \"\"\"\n",
    "    For each year and season, list images, activate assets in batch, download assets, \n",
    "    log to a single download_log.txt, and build a master image_roster.csv.\n",
    "    \n",
    "    Parameters:\n",
    "    - test_n_images: If > 0, only process the first n images for each season/year\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Starting download process at {datetime.now().isoformat()}\")\n",
    "    print(f\"Study site folder: {study_site_folder}\")\n",
    "    print(f\"Backup folder: {backup_folder}\")\n",
    "    print(f\"Cloud threshold: {cloud_threshold}\")\n",
    "    print(f\"Rate limit: {MAX_REQUESTS_PER_SECOND} requests per second\")\n",
    "    if test_n_images > 0:\n",
    "        print(f\"TEST MODE ACTIVE: Limited to first {test_n_images} images per season/year\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "    \n",
    "    # Single log file in base\n",
    "    log_path = os.path.join(study_site_folder, 'download_log.txt')\n",
    "    # For CSV: collect { order_name: [image_id, ...] }\n",
    "    roster_dict = {}\n",
    "\n",
    "    for year in years:\n",
    "        for season, (start_tmpl, end_tmpl) in seasons.items():\n",
    "            start = start_tmpl.format(year=year)\n",
    "            end = end_tmpl.format(year=year)\n",
    "            order_name = f\"{season}_{year}\"\n",
    "            print(f\"\\n{'=' * 60}\")\n",
    "            print(f\"Processing {order_name}: {start} to {end}\")\n",
    "            print(f\"{'=' * 60}\")\n",
    "\n",
    "            # 1) list & filter\n",
    "            feats = list_images(AOI, start, end, cloud_threshold)\n",
    "            image_ids = [f['id'] for f in feats]\n",
    "            roster_dict[order_name] = image_ids\n",
    "            print(f\"Found {len(image_ids)} images meeting criteria.\")\n",
    "            \n",
    "            if not feats:\n",
    "                print(f\"No images found for {order_name}, skipping to next season/year.\")\n",
    "                continue\n",
    "\n",
    "            # 2) choose storage\n",
    "            free = shutil.disk_usage(study_site_folder).free\n",
    "            if free < 5 * 1024**3:\n",
    "                base_folder = backup_folder\n",
    "                print(f\"WARNING: Less than 5 GB free space in primary folder\")\n",
    "                print(f\"Switching to backup folder: {backup_folder}\")\n",
    "            else:\n",
    "                base_folder = study_site_folder\n",
    "                print(f\"Using primary folder: {study_site_folder}\")\n",
    "                print(f\"Free space: {free / 1024**3:.2f} GB\")\n",
    "\n",
    "            # 3) activate assets in batch for the entire season\n",
    "            print(f\"\\nActivating assets for {order_name}...\")\n",
    "            t0_activation = time.time()\n",
    "            try:\n",
    "                asset_mapping = batch_activate_assets(feats, ASSET_KEYS, test_n_images)\n",
    "                activation_time = time.time() - t0_activation\n",
    "                print(f\"All assets activated for {order_name} in {activation_time:.1f} seconds\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during activation for {order_name}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # 4) download activated assets\n",
    "            t0_download = time.time()\n",
    "            try:\n",
    "                num_assets = download_assets_batch(feats, asset_mapping, base_folder, order_name, test_n_images)\n",
    "                download_time = time.time() - t0_download\n",
    "                total_time = time.time() - t0_activation\n",
    "                print(f\"Download complete for {order_name}\")\n",
    "                print(f\"Downloaded {num_assets} assets in {download_time:.1f} seconds\")\n",
    "                print(f\"Total processing time: {total_time:.1f} seconds\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR during download for {order_name}: {str(e)}\")\n",
    "                total_time = time.time() - t0_activation\n",
    "                num_assets = 0\n",
    "\n",
    "            # 5) append to log\n",
    "            print(f\"Updating download log at {log_path}\")\n",
    "            with open(log_path, 'a') as logf:\n",
    "                log_entry = (\n",
    "                    f\"{datetime.now().isoformat()} | {order_name} | \"\n",
    "                    f\"images={len(image_ids)} | assets={num_assets} | \"\n",
    "                    f\"time_s={total_time:.1f}\"\n",
    "                )\n",
    "                if test_n_images > 0:\n",
    "                    log_entry += f\" | test_n_images_override={test_n_images}\"\n",
    "                log_entry += \"\\n\"\n",
    "                logf.write(log_entry)\n",
    "\n",
    "    # 6) write master CSV\n",
    "    csv_path = os.path.join(study_site_folder, 'image_roster.csv')\n",
    "    print(f\"\\nGenerating master image roster at {csv_path}\")\n",
    "    all_orders = list(roster_dict.keys())\n",
    "    max_len = max(len(v) for v in roster_dict.values()) if roster_dict else 0\n",
    "\n",
    "    with open(csv_path, 'w', newline='') as csvf:\n",
    "        writer = csv.writer(csvf)\n",
    "        writer.writerow(all_orders)\n",
    "        for i in range(max_len):\n",
    "            row = [roster_dict[ord][i] if i < len(roster_dict[ord]) else ''\n",
    "                   for ord in all_orders]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Processing complete at {datetime.now().isoformat()}\")\n",
    "    print(f\"Log updated at: {log_path}\")\n",
    "    print(f\"Master roster CSV at: {csv_path}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "def extract_all_zips(folder_path):\n",
    "    \"\"\"\n",
    "    Extract all zip files found in the specified folder,\n",
    "    then delete the original zip files after successful extraction.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Starting zip extraction process for {folder_path} at {datetime.now().isoformat()}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder {folder_path} does not exist, skipping.\")\n",
    "        return\n",
    "    \n",
    "    # Find all zip files in the folder\n",
    "    zip_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) \n",
    "                if f.endswith('.zip') and os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    if not zip_files:\n",
    "        print(f\"No zip files found in {folder_path}.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(zip_files)} zip files to extract.\")\n",
    "    \n",
    "    # Process each zip file\n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for zip_path in zip_files:\n",
    "        try:\n",
    "            zip_name = os.path.basename(zip_path)\n",
    "            extract_folder = os.path.join(folder_path, os.path.splitext(zip_name)[0])\n",
    "            \n",
    "            print(f\"\\nProcessing: {zip_name}\")\n",
    "            print(f\"Extracting to: {extract_folder}\")\n",
    "            \n",
    "            # Create extraction folder\n",
    "            os.makedirs(extract_folder, exist_ok=True)\n",
    "            \n",
    "            # Extract the zip file\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                file_count = len(zip_ref.namelist())\n",
    "                print(f\"Zip contains {file_count} files\")\n",
    "                zip_ref.extractall(extract_folder)\n",
    "            \n",
    "            # Verify extraction\n",
    "            extracted_files = []\n",
    "            for root, _, files in os.walk(extract_folder):\n",
    "                extracted_files.extend(files)\n",
    "            \n",
    "            if len(extracted_files) == file_count:\n",
    "                print(f\"Extraction successful: {file_count} files extracted\")\n",
    "                print(f\"Deleting original zip file: {zip_path}\")\n",
    "                os.remove(zip_path)\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"WARNING: Extraction verification failed. Expected {file_count} files but found {len(extracted_files)}\")\n",
    "                print(f\"Original zip file {zip_path} preserved for manual inspection\")\n",
    "                fail_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR extracting {zip_path}: {str(e)}\")\n",
    "            print(f\"Original zip file preserved for manual inspection\")\n",
    "            fail_count += 1\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Extraction process completed for {folder_path} at {datetime.now().isoformat()}\")\n",
    "    print(f\"Successfully processed: {success_count} zip files\")\n",
    "    if fail_count > 0:\n",
    "        print(f\"Failed: {fail_count} zip files - see above for details\")\n",
    "    print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [[\n",
    "        [-162.369230, 60.945820],\n",
    "        [-162.375829, 61.394512],\n",
    "        [-161.440110, 61.394512],\n",
    "        [-161.446709, 60.945820],\n",
    "        [-162.369230, 60.945820],\n",
    "    ]]\n",
    "}\n",
    "\n",
    "years = [2023, 2024]\n",
    "seasons = {\n",
    "    \"Breakup\": (\"{year}-04-15T00:00:00Z\", \"{year}-06-15T23:59:59Z\"),\n",
    "    \"Freezeup\": (\"{year}-10-01T00:00:00Z\", \"{year}-11-30T23:59:59Z\")\n",
    "}\n",
    "\n",
    "primary = r\"D:\\planetscope_lake_ice\\Data\\Input\\YKD\"\n",
    "backup = r\"C:\\Users\\nj142\\Desktop\\Fallback\"\n",
    "\n",
    "download_seasonal_orders(\n",
    "    AOI=rect,\n",
    "    study_site_folder=primary,\n",
    "    backup_folder=backup,\n",
    "    years=years,\n",
    "    seasons=seasons,\n",
    "    cloud_threshold=0.5,\n",
    ")\n",
    "\n",
    "extract_all_zips(primary)\n",
    "extract_all_zips(backup)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
